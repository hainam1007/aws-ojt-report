[{"uri":"https://hainam1007.github.io/aws-ojt-report/4-eventparticipated/4.2-event2/","title":"AI/ML/GenAI on AWS","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AI/ML/GenAI on AWS Workshop\u0026rdquo; Event Information Event Name: AI/ML/GenAI on AWS Workshop\nDate: Saturday, November 15, 2025\nTime: 8:30 AM ‚Äì 12:00 PM\nLocation: AWS Vietnam Office\nDuration: 3.5 hours\nWorkshop Overview This hands-on workshop provided a comprehensive introduction to AWS\u0026rsquo;s artificial intelligence and machine learning services, with a special focus on Generative AI using Amazon Bedrock. The session combined theory with practical demonstrations, helping participants understand how to build AI-powered applications on AWS.\nKey Topics Covered 1. AWS AI Services The workshop introduced six core AWS AI services that solve common business problems:\nAmazon Rekognition ‚Äì Computer Vision\nAnalyze images and videos automatically Detect faces and compare them Identify objects, scenes, and activities Filter inappropriate content Amazon Polly ‚Äì Text-to-Speech\nConvert written text into natural speech Choose from various voices and languages Create voice-enabled applications Amazon Transcribe ‚Äì Speech-to-Text\nConvert audio to text automatically Works in real-time or batch mode Supports multiple languages Amazon Comprehend ‚Äì Natural Language Processing\nAnalyze text to understand sentiment Extract key information (names, places, dates) Detect the language used Amazon Translate ‚Äì Language Translation\nTranslate text between languages using AI Handle real-time or large batches Use custom vocabulary for specific industries Amazon Lex ‚Äì Conversational AI\nBuild chatbots for customer service Create voice assistants Connect with other AWS services easily 2. Generative AI with Amazon Bedrock What is Amazon Bedrock? Amazon Bedrock is AWS\u0026rsquo;s platform for building applications with foundation models (large AI models) without managing infrastructure.\nFoundation Models Comparison\nModel Strengths Best For Claude Strong reasoning, safety Complex tasks, content creation Llama Open-source, customizable Custom solutions, cost-sensitive Titan AWS-native, reliable General AWS integration Prompt Engineering Basics\nLearning how to write effective prompts to get better AI responses:\nClear instructions: Be specific about what you want Chain-of-Thought: Ask the AI to explain its reasoning step-by-step Few-shot learning: Give examples of the output you want Example:\nBad prompt: \u0026#34;Write about cats\u0026#34; Good prompt: \u0026#34;Write a 200-word article about cat behavior for pet owners, focusing on why cats knock things over\u0026#34; Retrieval-Augmented Generation (RAG)\nRAG helps AI models use your own data to generate more accurate answers:\nStore your documents in a knowledge base When a user asks a question, find relevant documents Send both the question and documents to the AI Get an accurate, context-aware answer Bedrock Agents\nCreate AI agents that can:\nBreak down complex tasks into steps Use tools and APIs to accomplish tasks Make decisions based on information Guardrails\nProtect your AI applications with:\nContent filtering (block harmful content) Topic restrictions (stay on approved topics) Sensitive information masking (hide personal data) 3. Live Demo The highlight was building a Generative AI chatbot using Amazon Bedrock:\nConnected to a foundation model (Claude) Added a knowledge base with company documents Implemented guardrails for safety Tested real conversations What I Learned Understanding AWS AI Services Before the workshop, I wasn\u0026rsquo;t sure which AWS service to use for different AI tasks. Now I know:\nUse Rekognition for analyzing images/videos Use Polly to add voice to applications Use Comprehend to understand customer feedback Use Bedrock for custom AI applications Practical Prompt Engineering Learning prompt engineering was eye-opening. Small changes in how you ask questions can dramatically improve AI responses. The Chain-of-Thought technique is particularly useful for complex problems.\nRAG Architecture Benefits Understanding RAG showed me how to make AI applications that:\nUse company-specific information Stay up-to-date without retraining Give more accurate and relevant answers Building Safe AI Applications The guardrails session taught me the importance of:\nPreventing harmful content generation Protecting user privacy Keeping AI responses on-topic Skills Gained After this workshop, I can now:\n‚úÖ Choose the right AWS AI service for different use cases\n‚úÖ Compare foundation models and select appropriate ones\n‚úÖ Write effective prompts for better AI responses\n‚úÖ Understand RAG architecture and when to use it\n‚úÖ Build a basic Generative AI chatbot with Bedrock\n‚úÖ Implement safety measures using Guardrails\nHow I\u0026rsquo;ll Apply This Short-term Applications Experiment with different foundation models on Bedrock Practice prompt engineering techniques Build a simple chatbot prototype for testing Long-term Goals Propose an AI-powered feature for our current project Implement RAG for a knowledge base search system Integrate AWS AI services into existing applications Event Experience The workshop was well-organized and beginner-friendly. The speakers explained complex concepts clearly and the hands-on demo made everything practical. Having 3.5 hours was perfect‚Äîenough time to cover important topics without feeling rushed.\nThe AWS Vietnam office provided a great learning environment with good facilities and networking opportunities with other participants interested in AI/ML.\nWhat I Liked Hands-on approach with live demos Clear comparison of different AI services Practical examples from real projects Access to workshop materials for future reference Event Photos Add your event photos here\nResources Provided AWS AI Services documentation links Bedrock getting started guide Prompt engineering cheat sheet RAG implementation examples Sample code from the chatbot demo Conclusion This workshop gave me a solid foundation in AWS AI/ML services and Generative AI. The combination of service overview, prompt engineering, RAG architecture, and hands-on demos provided both breadth and depth of knowledge.\nI\u0026rsquo;m excited to explore these tools further and find ways to integrate AI capabilities into my projects. The workshop demonstrated that building AI applications on AWS is more accessible than I thought.\nFor more information about AWS AI services, visit: AWS AI Services\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-deploybackend/5.3.1-architecture/","title":"Architecture Deep Dive","tags":[],"description":"","content":"Let\u0026rsquo;s dive into the BackendStack source code to understand how the system operates. Below are the detailed code snippets for each resource.\n1. Database (Amazon DynamoDB) We initialize 6 DynamoDB tables using PAY_PER_REQUEST (On-Demand) mode to optimize costs and scalability.\n// 1. Listings Table (Room rentals) const listingsTable = new dynamodb.Table(this, \u0026#34;ListingsTable\u0026#34;, { tableName: \u0026#34;BoardingHouseListings\u0026#34;, partitionKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 2. User Profiles Table const userProfilesTable = new dynamodb.Table(this, \u0026#34;UserProfilesTable\u0026#34;, { tableName: \u0026#34;UserProfiles\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 3. OTP Table (Phone Verification) - Auto-delete after 5 mins (TTL) const otpTable = new dynamodb.Table(this, \u0026#34;OTPVerifications\u0026#34;, { tableName: \u0026#34;OTPVerifications\u0026#34;, partitionKey: { name: \u0026#34;phoneNumber\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, // TTL configuration removalPolicy: RemovalPolicy.DESTROY, }); // 4. Favorites Table - Includes Sort Key for quick lookup by user const favoritesTable = new dynamodb.Table(this, \u0026#34;FavoritesTable\u0026#34;, { tableName: \u0026#34;UserFavorites\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 5. Support Requests Table const supportRequestsTable = new dynamodb.Table(this, \u0026#34;SupportRequestsTable\u0026#34;, { tableName: \u0026#34;SupportRequests\u0026#34;, partitionKey: { name: \u0026#34;requestId\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 6. Notifications Table - Includes TTL const notificationsTable = new dynamodb.Table(this, \u0026#34;NotificationsTable\u0026#34;, { tableName: \u0026#34;Notifications\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;notificationId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, removalPolicy: RemovalPolicy.DESTROY, }); 2. Storage (Amazon S3) An S3 Bucket is created to store room images, configured with security blocks against public access and auto-deletion when the stack is destroyed.\nconst imagesBucket = new s3.Bucket(this, \u0026#34;BoardingHouseImages\u0026#34;, { bucketName: `findnest-images-${cdk.Aws.ACCOUNT_ID}`, // Unique bucket name removalPolicy: RemovalPolicy.DESTROY, autoDeleteObjects: true, blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL, // Private by default }); 3. Authentication (Amazon Cognito) We configure a User Pool to manage users and an Identity Pool to grant the Frontend direct access to AWS resources.\n// Create User Pool const userPool = new cognito.UserPool(this, \u0026#34;UserPool\u0026#34;, { userPoolName: \u0026#34;FindNestUsers\u0026#34;, selfSignUpEnabled: false, // User created via API (Backend trigger) signInAliases: { phone: true, // User uses Phone Number username: true, // Admin uses Username }, autoVerify: { phone: true }, standardAttributes: { email: { required: false, mutable: true }, phoneNumber: { required: false, mutable: true }, }, passwordPolicy: { minLength: 8, requireLowercase: true, requireUppercase: true, requireDigits: true, requireSymbols: true, }, accountRecovery: cognito.AccountRecovery.PHONE_ONLY_WITHOUT_MFA, removalPolicy: RemovalPolicy.DESTROY, }); // Create Client App const userPoolClient = userPool.addClient(\u0026#34;UserPoolClient\u0026#34;, { authFlows: { userPassword: true, adminUserPassword: true, // Used for Backend auth flow custom: true, }, }); // User Groups const usersGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;UsersGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Users\u0026#34;, }); const landlordsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;LandlordsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Landlords\u0026#34;, }); const adminsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;AdminsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Admins\u0026#34;, }); // Identity Pool for Frontend const identityPool = new cognito.CfnIdentityPool(this, \u0026#34;IdentityPool\u0026#34;, { identityPoolName: \u0026#34;FindNestMapAccess\u0026#34;, allowUnauthenticatedIdentities: true, // Allow guests to view map cognitoIdentityProviders: [ { clientId: userPoolClient.userPoolClientId, providerName: userPool.userPoolProviderName, }, ], }); // IAM Role for unauthenticated users (map access only) const unauthenticatedRole = new iam.Role(this, \u0026#34;UnauthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;unauthenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // IAM Role for authenticated users (full access) const authenticatedRole = new iam.Role(this, \u0026#34;AuthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;authenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, \u0026#34;geo:BatchGetDevicePosition\u0026#34;, \u0026#34;geo:GetDevicePosition\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // Attach roles to identity pool new cognito.CfnIdentityPoolRoleAttachment(this, \u0026#34;IdentityPoolRoleAttachment\u0026#34;, { identityPoolId: identityPool.ref, roles: { authenticated: authenticatedRole.roleArn, unauthenticated: unauthenticatedRole.roleArn, }, }); 4. Location Service (Maps \u0026amp; Geocoding) Initialize Location Service resources using Here data provider for better POI coverage in Vietnam.\nconst placeIndex = new location.CfnPlaceIndex(this, \u0026#34;PlaceIndex\u0026#34;, { indexName: `FindNestPlacesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, // Better POI coverage for Asia (Vietnam) dataSourceConfiguration: { intendedUse: \u0026#34;Storage\u0026#34;, // Allows storing and querying POI data }, }); const map = new location.CfnMap(this, \u0026#34;Map\u0026#34;, { mapName: `FindNestMap-${cdk.Aws.ACCOUNT_ID}`, configuration: { style: \u0026#34;VectorEsriStreets\u0026#34; }, }); const routeCalculator = new location.CfnRouteCalculator( this, \u0026#34;RouteCalculator\u0026#34;, { calculatorName: `FindNestRoutesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, } ); 5. Compute (AWS Lambda Monolith) Configure the Lambda Function containing the entire Backend logic, including full environment variable injection.\nconst apiLambda = new lambda.Function(this, \u0026#34;ApiLambda\u0026#34;, { functionName: \u0026#34;FindNestApi\u0026#34;, runtime: lambda.Runtime.NODEJS_20_X, handler: \u0026#34;index.handler\u0026#34;, code: lambda.Code.fromAsset(path.join(__dirname, \u0026#34;../../backend/src/lambda\u0026#34;)), timeout: cdk.Duration.seconds(30), logGroup: logGroup, environment: { // Environment variables connecting resources LISTINGS_TABLE_NAME: listingsTable.tableName, USER_PROFILES_TABLE_NAME: userProfilesTable.tableName, OTP_TABLE_NAME: otpTable.tableName, FAVORITES_TABLE_NAME: favoritesTable.tableName, SUPPORT_REQUESTS_TABLE_NAME: supportRequestsTable.tableName, NOTIFICATIONS_TABLE_NAME: notificationsTable.tableName, IMAGES_BUCKET_NAME: imagesBucket.bucketName, USER_POOL_ID: userPool.userPoolId, USER_POOL_CLIENT_ID: userPoolClient.userPoolClientId, PLACE_INDEX_NAME: placeIndex.indexName, MAP_NAME: map.mapName, ROUTE_CALCULATOR_NAME: routeCalculator.calculatorName, BEDROCK_MODEL_ID: \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34;, REGION: cdk.Aws.REGION, }, }); 6. Permissions (Granular Access Control) We grant Granular Permissions to the Lambda Function. Here is the full list of granted permissions:\nA. Read/Write Access to Database and S3:\nlistingsTable.grantReadWriteData(apiLambda); userProfilesTable.grantReadWriteData(apiLambda); otpTable.grantReadWriteData(apiLambda); favoritesTable.grantReadWriteData(apiLambda); supportRequestsTable.grantReadWriteData(apiLambda); notificationsTable.grantReadWriteData(apiLambda); imagesBucket.grantReadWrite(apiLambda); B. User Management in Cognito (Full Admin Actions):\napiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminAddUserToGroup\u0026#34;, \u0026#34;cognito-idp:AdminRemoveUserFromGroup\u0026#34;, \u0026#34;cognito-idp:AdminListGroupsForUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:AdminEnableUser\u0026#34;, \u0026#34;cognito-idp:AdminDisableUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:GlobalSignOut\u0026#34;, ], resources: [userPool.userPoolArn], }) ); C. Integrations with other services (SNS, Bedrock, Location):\n// Send SMS (OTP) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;sns:Publish\u0026#34;], resources: [\u0026#34;*\u0026#34;], }) ); // Invoke AI (Claude 3) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;bedrock:InvokeModel\u0026#34;], resources: [\u0026#34;arn:aws:bedrock:*::foundation-model/anthropic.claude-3-*\u0026#34;], }) ); // Access Location Service apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, ], resources: [placeIndex.attrArn, routeCalculator.attrArn], }) ); 7. API Gateway (REST API) Create a public Endpoint for clients to call the Lambda function.\nconst api = new apigateway.LambdaRestApi(this, \u0026#34;BoardingHouseApi\u0026#34;, { handler: apiLambda, proxy: true, deployOptions: { stageName: \u0026#34;prod\u0026#34;, throttlingBurstLimit: 100, throttlingRateLimit: 50, }, defaultCorsPreflightOptions: { allowOrigins: apigateway.Cors.ALL_ORIGINS, allowMethods: apigateway.Cors.ALL_METHODS, allowHeaders: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;], }, restApiName: \u0026#34;FindNestAPI\u0026#34;, }); 8. Monitoring \u0026amp; Observability (CloudWatch) We implement comprehensive monitoring with CloudWatch Dashboard, Alarms, and SNS Notifications.\nA. SNS Alert Topic:\nconst alertTopic = new sns.Topic(this, \u0026#34;AlertTopic\u0026#34;, { topicName: \u0026#34;BoardingHouseAlerts\u0026#34;, displayName: \u0026#34;Smart Boarding House Alerts\u0026#34;, }); alertTopic.addSubscription( new snsSubscriptions.EmailSubscription(\u0026#34;admin@smartboardinghouse.com\u0026#34;) ); B. CloudWatch Alarms:\n// Lambda Error Alarm const lambdaErrorAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaErrorAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Errors\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 5, evaluationPeriods: 2, }); // Lambda Duration Alarm const lambdaDurationAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaDurationAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Duration\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Duration\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Average\u0026#34;, }), threshold: 25000, // 25 seconds evaluationPeriods: 3, }); // API Gateway 4xx/5xx Alarms const apiGateway4xxAlarm = new cloudwatch.Alarm(this, \u0026#34;ApiGateway4xxAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-API-4xx-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/ApiGateway\u0026#34;, metricName: \u0026#34;4XXError\u0026#34;, dimensionsMap: { ApiName: apiGatewayName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 10, evaluationPeriods: 2, }); C. CloudWatch Dashboard (8 Rows):\nconst dashboard = new cloudwatch.Dashboard(this, \u0026#34;BoardingHouseDashboard\u0026#34;, { dashboardName: \u0026#34;SmartBoardingHouse-Monitoring\u0026#34;, defaultInterval: cdk.Duration.hours(24), }); // Row 1: Lambda Overview (Invocations, Errors, Throttles) dashboard.addWidgets( new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Invocations\u0026#34;, left: [lambdaInvocationsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Errors\u0026#34;, left: [lambdaErrorsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Throttles\u0026#34;, left: [lambdaThrottlesMetric], width: 8, height: 6, }) ); // Row 2: Lambda Performance (Duration, Concurrent Executions) // Row 3: API Gateway (Requests, Latency) // Row 4: API Gateway Errors (4XX, 5XX) // Row 5: DynamoDB Metrics (Read/Write Capacity, Errors) // Row 6: System Health Summary (Error Rates, Response Time, Total Requests) // Row 7: Bedrock AI Token Usage \u0026amp; Invocations // Row 8: Bedrock Model Summary (Total Tokens, Invocations, Latency) Dashboard Features:\nüìä Lambda Metrics: Invocations, Errors, Throttles, Duration, Concurrency üåê API Gateway Metrics: Requests, Latency (Avg \u0026amp; P99), 4XX/5XX Errors üíæ DynamoDB Metrics: Read/Write Capacity, User Errors per table ü§ñ Bedrock AI Metrics: Token Usage (Input/Output), Invocations, Latency üìà System Health Summary: Error Rates, Response Time, Total Requests (24h) üö® Automatic Alerts: Email notifications via SNS when thresholds are breached "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Architecture Deep Dive","tags":[],"description":"","content":"Let\u0026rsquo;s dive into the BackendStack source code to understand how the system operates. Below are the detailed code snippets for each resource.\n1. Database (Amazon DynamoDB) We initialize 6 DynamoDB tables using PAY_PER_REQUEST (On-Demand) mode to optimize costs and scalability.\n// 1. Listings Table (Room rentals) const listingsTable = new dynamodb.Table(this, \u0026#34;ListingsTable\u0026#34;, { tableName: \u0026#34;BoardingHouseListings\u0026#34;, partitionKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 2. User Profiles Table const userProfilesTable = new dynamodb.Table(this, \u0026#34;UserProfilesTable\u0026#34;, { tableName: \u0026#34;UserProfiles\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 3. OTP Table (Phone Verification) - Auto-delete after 5 mins (TTL) const otpTable = new dynamodb.Table(this, \u0026#34;OTPVerifications\u0026#34;, { tableName: \u0026#34;OTPVerifications\u0026#34;, partitionKey: { name: \u0026#34;phoneNumber\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, // TTL configuration removalPolicy: RemovalPolicy.DESTROY, }); // 4. Favorites Table - Includes Sort Key for quick lookup by user const favoritesTable = new dynamodb.Table(this, \u0026#34;FavoritesTable\u0026#34;, { tableName: \u0026#34;UserFavorites\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 5. Support Requests Table const supportRequestsTable = new dynamodb.Table(this, \u0026#34;SupportRequestsTable\u0026#34;, { tableName: \u0026#34;SupportRequests\u0026#34;, partitionKey: { name: \u0026#34;requestId\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 6. Notifications Table - Includes TTL const notificationsTable = new dynamodb.Table(this, \u0026#34;NotificationsTable\u0026#34;, { tableName: \u0026#34;Notifications\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;notificationId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, removalPolicy: RemovalPolicy.DESTROY, }); 2. Storage (Amazon S3) An S3 Bucket is created to store room images, configured with security blocks against public access and auto-deletion when the stack is destroyed.\nconst imagesBucket = new s3.Bucket(this, \u0026#34;BoardingHouseImages\u0026#34;, { bucketName: `findnest-images-${cdk.Aws.ACCOUNT_ID}`, // Unique bucket name removalPolicy: RemovalPolicy.DESTROY, autoDeleteObjects: true, blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL, // Private by default }); 3. Authentication (Amazon Cognito) We configure a User Pool to manage users and an Identity Pool to grant the Frontend direct access to AWS resources.\n// Create User Pool const userPool = new cognito.UserPool(this, \u0026#34;UserPool\u0026#34;, { userPoolName: \u0026#34;FindNestUsers\u0026#34;, selfSignUpEnabled: false, // User created via API (Backend trigger) signInAliases: { phone: true, // User uses Phone Number username: true, // Admin uses Username }, autoVerify: { phone: true }, standardAttributes: { email: { required: false, mutable: true }, phoneNumber: { required: false, mutable: true }, }, passwordPolicy: { minLength: 8, requireLowercase: true, requireUppercase: true, requireDigits: true, requireSymbols: true, }, accountRecovery: cognito.AccountRecovery.PHONE_ONLY_WITHOUT_MFA, removalPolicy: RemovalPolicy.DESTROY, }); // Create Client App const userPoolClient = userPool.addClient(\u0026#34;UserPoolClient\u0026#34;, { authFlows: { userPassword: true, adminUserPassword: true, // Used for Backend auth flow custom: true, }, }); // User Groups const usersGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;UsersGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Users\u0026#34;, }); const landlordsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;LandlordsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Landlords\u0026#34;, }); const adminsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;AdminsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Admins\u0026#34;, }); // Identity Pool for Frontend const identityPool = new cognito.CfnIdentityPool(this, \u0026#34;IdentityPool\u0026#34;, { identityPoolName: \u0026#34;FindNestMapAccess\u0026#34;, allowUnauthenticatedIdentities: true, // Allow guests to view map cognitoIdentityProviders: [ { clientId: userPoolClient.userPoolClientId, providerName: userPool.userPoolProviderName, }, ], }); // IAM Role for unauthenticated users (map access only) const unauthenticatedRole = new iam.Role(this, \u0026#34;UnauthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;unauthenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // IAM Role for authenticated users (full access) const authenticatedRole = new iam.Role(this, \u0026#34;AuthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;authenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, \u0026#34;geo:BatchGetDevicePosition\u0026#34;, \u0026#34;geo:GetDevicePosition\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // Attach roles to identity pool new cognito.CfnIdentityPoolRoleAttachment(this, \u0026#34;IdentityPoolRoleAttachment\u0026#34;, { identityPoolId: identityPool.ref, roles: { authenticated: authenticatedRole.roleArn, unauthenticated: unauthenticatedRole.roleArn, }, }); 4. Location Service (Maps \u0026amp; Geocoding) Initialize Location Service resources using Here data provider for better POI coverage in Vietnam.\nconst placeIndex = new location.CfnPlaceIndex(this, \u0026#34;PlaceIndex\u0026#34;, { indexName: `FindNestPlacesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, // Better POI coverage for Asia (Vietnam) dataSourceConfiguration: { intendedUse: \u0026#34;Storage\u0026#34;, // Allows storing and querying POI data }, }); const map = new location.CfnMap(this, \u0026#34;Map\u0026#34;, { mapName: `FindNestMap-${cdk.Aws.ACCOUNT_ID}`, configuration: { style: \u0026#34;VectorEsriStreets\u0026#34; }, }); const routeCalculator = new location.CfnRouteCalculator( this, \u0026#34;RouteCalculator\u0026#34;, { calculatorName: `FindNestRoutesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, } ); 5. Compute (AWS Lambda Monolith) Configure the Lambda Function containing the entire Backend logic, including full environment variable injection.\nconst apiLambda = new lambda.Function(this, \u0026#34;ApiLambda\u0026#34;, { functionName: \u0026#34;FindNestApi\u0026#34;, runtime: lambda.Runtime.NODEJS_20_X, handler: \u0026#34;index.handler\u0026#34;, code: lambda.Code.fromAsset(path.join(__dirname, \u0026#34;../../backend/src/lambda\u0026#34;)), timeout: cdk.Duration.seconds(30), logGroup: logGroup, environment: { // Environment variables connecting resources LISTINGS_TABLE_NAME: listingsTable.tableName, USER_PROFILES_TABLE_NAME: userProfilesTable.tableName, OTP_TABLE_NAME: otpTable.tableName, FAVORITES_TABLE_NAME: favoritesTable.tableName, SUPPORT_REQUESTS_TABLE_NAME: supportRequestsTable.tableName, NOTIFICATIONS_TABLE_NAME: notificationsTable.tableName, IMAGES_BUCKET_NAME: imagesBucket.bucketName, USER_POOL_ID: userPool.userPoolId, USER_POOL_CLIENT_ID: userPoolClient.userPoolClientId, PLACE_INDEX_NAME: placeIndex.indexName, MAP_NAME: map.mapName, ROUTE_CALCULATOR_NAME: routeCalculator.calculatorName, BEDROCK_MODEL_ID: \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34;, REGION: cdk.Aws.REGION, }, }); 6. Permissions (Granular Access Control) We grant Granular Permissions to the Lambda Function. Here is the full list of granted permissions:\nA. Read/Write Access to Database and S3:\nlistingsTable.grantReadWriteData(apiLambda); userProfilesTable.grantReadWriteData(apiLambda); otpTable.grantReadWriteData(apiLambda); favoritesTable.grantReadWriteData(apiLambda); supportRequestsTable.grantReadWriteData(apiLambda); notificationsTable.grantReadWriteData(apiLambda); imagesBucket.grantReadWrite(apiLambda); B. User Management in Cognito (Full Admin Actions):\napiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminAddUserToGroup\u0026#34;, \u0026#34;cognito-idp:AdminRemoveUserFromGroup\u0026#34;, \u0026#34;cognito-idp:AdminListGroupsForUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:AdminEnableUser\u0026#34;, \u0026#34;cognito-idp:AdminDisableUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:GlobalSignOut\u0026#34;, ], resources: [userPool.userPoolArn], }) ); C. Integrations with other services (SNS, Bedrock, Location):\n// Send SMS (OTP) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;sns:Publish\u0026#34;], resources: [\u0026#34;*\u0026#34;], }) ); // Invoke AI (Claude 3) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;bedrock:InvokeModel\u0026#34;], resources: [\u0026#34;arn:aws:bedrock:*::foundation-model/anthropic.claude-3-*\u0026#34;], }) ); // Access Location Service apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, ], resources: [placeIndex.attrArn, routeCalculator.attrArn], }) ); 7. API Gateway (REST API) Create a public Endpoint for clients to call the Lambda function.\nconst api = new apigateway.LambdaRestApi(this, \u0026#34;BoardingHouseApi\u0026#34;, { handler: apiLambda, proxy: true, deployOptions: { stageName: \u0026#34;prod\u0026#34;, throttlingBurstLimit: 100, throttlingRateLimit: 50, }, defaultCorsPreflightOptions: { allowOrigins: apigateway.Cors.ALL_ORIGINS, allowMethods: apigateway.Cors.ALL_METHODS, allowHeaders: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;], }, restApiName: \u0026#34;FindNestAPI\u0026#34;, }); 8. Monitoring \u0026amp; Observability (CloudWatch) We implement comprehensive monitoring with CloudWatch Dashboard, Alarms, and SNS Notifications.\nA. SNS Alert Topic:\nconst alertTopic = new sns.Topic(this, \u0026#34;AlertTopic\u0026#34;, { topicName: \u0026#34;BoardingHouseAlerts\u0026#34;, displayName: \u0026#34;Smart Boarding House Alerts\u0026#34;, }); alertTopic.addSubscription( new snsSubscriptions.EmailSubscription(\u0026#34;admin@smartboardinghouse.com\u0026#34;) ); B. CloudWatch Alarms:\n// Lambda Error Alarm const lambdaErrorAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaErrorAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Errors\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 5, evaluationPeriods: 2, }); // Lambda Duration Alarm const lambdaDurationAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaDurationAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Duration\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Duration\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Average\u0026#34;, }), threshold: 25000, // 25 seconds evaluationPeriods: 3, }); // API Gateway 4xx/5xx Alarms const apiGateway4xxAlarm = new cloudwatch.Alarm(this, \u0026#34;ApiGateway4xxAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-API-4xx-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/ApiGateway\u0026#34;, metricName: \u0026#34;4XXError\u0026#34;, dimensionsMap: { ApiName: apiGatewayName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 10, evaluationPeriods: 2, }); C. CloudWatch Dashboard (8 Rows):\nconst dashboard = new cloudwatch.Dashboard(this, \u0026#34;BoardingHouseDashboard\u0026#34;, { dashboardName: \u0026#34;SmartBoardingHouse-Monitoring\u0026#34;, defaultInterval: cdk.Duration.hours(24), }); // Row 1: Lambda Overview (Invocations, Errors, Throttles) dashboard.addWidgets( new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Invocations\u0026#34;, left: [lambdaInvocationsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Errors\u0026#34;, left: [lambdaErrorsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Throttles\u0026#34;, left: [lambdaThrottlesMetric], width: 8, height: 6, }) ); // Row 2: Lambda Performance (Duration, Concurrent Executions) // Row 3: API Gateway (Requests, Latency) // Row 4: API Gateway Errors (4XX, 5XX) // Row 5: DynamoDB Metrics (Read/Write Capacity, Errors) // Row 6: System Health Summary (Error Rates, Response Time, Total Requests) // Row 7: Bedrock AI Token Usage \u0026amp; Invocations // Row 8: Bedrock Model Summary (Total Tokens, Invocations, Latency) Dashboard Features:\nüìä Lambda Metrics: Invocations, Errors, Throttles, Duration, Concurrency üåê API Gateway Metrics: Requests, Latency (Avg \u0026amp; P99), 4XX/5XX Errors üíæ DynamoDB Metrics: Read/Write Capacity, User Errors per table ü§ñ Bedrock AI Metrics: Token Usage (Input/Output), Invocations, Latency üìà System Health Summary: Error Rates, Response Time, Total Requests (24h) üö® Automatic Alerts: Email notifications via SNS when thresholds are breached "},{"uri":"https://hainam1007.github.io/aws-ojt-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuild Agentic Systems with CrewAI and Amazon Bedrock By Tony Kipkemboi, Jo√£o (Joe) Moura, Karan Singh, and Aris Tsakpinis\n(co-authored with Joao Moura and Tony Kipkemboi from CrewAI)\n1. Introduction The enterprise AI landscape is undergoing a seismic shift as agentic systems move from experiments to mission‚Äëcritical business assets.\nDeloitte predicts that 25% of enterprises using generative AI will deploy AI agents in 2025, growing to 50% by 2027. The global AI agent market is projected to grow from $5.1B (2024) to $47.1B (2030). This post explains how CrewAI‚Äôs open source agentic framework combined with Amazon Bedrock enables sophisticated multi‚Äëagent systems that can transform business operations.\nWe focus on:\nWhat AI agents and agentic design are CrewAI concepts (flows, crews, agents, tasks, tools, process) How CrewAI integrates with Amazon Bedrock (including Agents and Knowledge Bases) A concrete solution for cloud security posture management on AWS 2. Agentic Design An AI agent is an autonomous, intelligent system that uses LLMs and other AI capabilities to perform complex tasks with minimal human oversight.\nCharacteristics:\nOperates independently and adapts to changing conditions Uses modular components: reasoning engines, memory, cognitive skills, tools Executes sophisticated workflows instead of fixed rule‚Äëbased logic Traditional SaaS solutions:\nAre horizontally scalable and generic Handle repetitive tasks well across sectors But lack deep domain intelligence and flexibility in dynamic environments Agentic systems bridge this gap by combining:\nContext‚Äëaware reasoning Domain knowledge Access to external tools and data sources Examples:\nSoftware development:\nCrewAI agents generate, evaluate, and improve code. In the CrewAI GitHub repo, pull requests are reviewed by agents checking documentation consistency and security. Supply chain management:\nTraditional systems only track stock; agentic systems use real‚Äëtime data (weather, geopolitical risk) to reroute shipments and reallocate resources. 3. Overview of CrewAI CrewAI is an enterprise suite and Python‚Äëbased open source framework for:\nBuilding AI flows, multi‚Äëagent systems, or combinations of both Allowing agents to work together through collaborative intelligence Key ideas:\nEach agent has a role, goal, and backstory, and can access specific tools Agents can delegate tasks and ask each other questions Emphasis on team collaboration, modular design, and simplicity The framework is used across industries (healthcare, finance, retail) to automate routine tasks and also create new, higher‚Äëskill roles.\n4. CrewAI Key Concepts At a high level, CrewAI offers two main ways to build agentic automations:\nFlows Crews 4.1 Flows CrewAI Flows provide a structured, event‚Äëdriven framework to orchestrate complex, multi‚Äëstep automations.\nFlows allow you to mix:\nRegular Python code Single LLM calls Multiple crews ‚Ä¶with:\nConditional logic Loops Real‚Äëtime state management With Amazon Bedrock as the LLM backend, flows can:\nRoute customer support tickets to different agent crews Coordinate financial analysis workflows that react to market volatility Together, Flows + Bedrock enable dynamic automations that adapt to changing conditions.\n4.2 Crews A Crew is a group of agents working together on related tasks.\nAgents Agents are autonomous entities defined by:\nRole ‚Äì function and responsibility in the system Backstory ‚Äì contextual narrative to guide decisions Goals ‚Äì objectives to achieve Tools ‚Äì APIs, databases, external systems they can use Agents can:\nCommunicate and collaborate Delegate tasks Use tools to execute complex workflows Tasks Tasks specify work to be done:\nDescription ‚Äì natural‚Äëlanguage description of the action Agent assignment ‚Äì which agent is responsible Tasks can be independent or part of larger workflows across multiple agents.\nTools Tools extend agent capabilities beyond pure LLM reasoning, for example:\nCalling REST APIs Querying databases Executing scripts Scraping websites Tools are modular and can be assigned to specific agents.\nProcess The process layer controls:\nHow agents coordinate Task execution order Communication and synchronization It ensures multi‚Äëagent workflows run smoothly end‚Äëto‚Äëend.\n5. CrewAI Enterprise Suite CrewAI also offers an enterprise product with:\nDedicated support and customization Integration with enterprise systems such as Amazon Bedrock Comprehensive monitoring and observability: Logs of agent interactions Performance metrics Dashboards to track workflows and optimize agents in real time This allows organizations to deploy AI agents at scale while maintaining security and compliance.\n6. Real‚ÄëWorld Enterprise Impact 6.1 Legacy Code Modernization A large enterprise needed to modernize legacy ABAP and APEX code.\nMultiple agents:\nAnalyze existing code components Generate modernized code in real time Execute tests in production Provide immediate feedback Results:\n~70% improvement in code generation speed Quality maintained via automated testing and feedback loops Solution containerized with Docker for scalable deployment 6.2 Back‚ÄëOffice Automation at a Global CPG A leading CPG company connected existing apps and data stores to CrewAI agents that:\nResearch industry conditions Analyze pricing data Summarize findings Execute decisions automatically Outcome:\n75% reduction in processing time End‚Äëto‚Äëend workflow from data analysis to action execution fully automated 7. Getting Started with CrewAI and Amazon Bedrock Amazon Bedrock integration lets CrewAI use managed foundation models as the LLM for agents.\n7.1 Basic LLM Configuration from crewai import Agent, Crew, Process, Task, LLM from crewai_tools import SerperDevTool, ScrapeWebsiteTool import os # Configure Bedrock LLM llm = LLM( model=\u0026#34;bedrock/anthropic.anthropic.claude-3-5-sonnet-20241022-v2:0\u0026#34;, aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;), aws_region_name=os.getenv(\u0026#39;AWS_REGION_NAME\u0026#39;) ) # Create an agent with Bedrock as the LLM provider security_analyst = Agent( config=agents_config[\u0026#39;security_analyst\u0026#39;], tools=[SerperDevTool(), ScrapeWebsiteTool()], llm=llm ) Amazon Bedrock advantages for CrewAI:\nAccess to state‚Äëof‚Äëthe‚Äëart models (Anthropic Claude, Amazon Nova, etc.) Enterprise‚Äëgrade security and compliance Scalability and reliability on AWS infrastructure 8. Using Amazon Bedrock Agents and Knowledge Bases Amazon Bedrock Agents ‚Äì fully managed, serverless autonomous agents.\nBedrockInvokeAgentTool lets CrewAI agents invoke Bedrock agents inside workflows. Amazon Bedrock Knowledge Bases ‚Äì securely connect FMs and agents to company data.\nBedrockKBRetrieverTool enables natural‚Äëlanguage retrieval from Knowledge Bases. 8.1 Bedrock Agents Example from crewai import Agent, Task, Crew from crewai_tools.aws.bedrock.agents.invoke_agent_tool import BedrockInvokeAgentTool # Initialize the Bedrock Agents tool agent_tool = BedrockInvokeAgentTool( agent_id=\u0026#34;your-agent-id\u0026#34;, agent_alias_id=\u0026#34;your-agent-alias-id\u0026#34; ) # CrewAI agent using the Bedrock Agents tool aws_expert = Agent( role=\u0026#39;AWS Service Expert\u0026#39;, goal=\u0026#39;Help users understand AWS services and quotas\u0026#39;, backstory=\u0026#39;I am an expert in AWS services and can provide detailed information about them.\u0026#39;, tools=[agent_tool], verbose=True ) 8.2 Knowledge Bases Example # Create and configure the BedrockKB tool kb_tool = BedrockKBRetrieverTool( knowledge_base_id=\u0026#34;your-kb-id\u0026#34;, number_of_results=5 ) # CrewAI agent using the Knowledge Base tool researcher = Agent( role=\u0026#39;Knowledge Base Researcher\u0026#39;, goal=\u0026#39;Find information about company policies\u0026#39;, backstory=\u0026#39;I am a researcher specialized in retrieving and analyzing company documentation.\u0026#39;, tools=[kb_tool], verbose=True ) 9. Operational Excellence and Observability Agentic applications combine deterministic and probabilistic components, so observability is crucial at three levels:\nApplication level ‚Äì overall system health and orchestration Model level ‚Äì LLM performance (accuracy, latency, throughput) Agent level ‚Äì behavior of individual and multi‚Äëagent workflows On AWS you can leverage:\nAmazon CloudWatch for application and model logs/metrics CrewAI‚Äôs own logging (basic by default, verbose=True for more detail) Third‚Äëparty tools such as AgentOps, Arize, MLFlow, LangFuse for advanced tracing and debugging 10. Solution Overview: Cloud Security Posture Management Use case: Cloud Security Posture Management (CSPM) for AWS.\nGoal: Continuously scan cloud infrastructure for misconfigurations and compliance risks using a team of three agents:\nInfrastructure mapper ‚Äì maps AWS resources and configurations Security analyst ‚Äì analyzes vulnerabilities and best practices Report writer ‚Äì generates clear, prioritized remediation reports 10.1 Implementation Steps (High Level) Step 1 ‚Äì Configure the Amazon Bedrock LLM\nfrom crewai import Agent, Crew, Process, Task, LLM from crewai.project import CrewBase, agent, crew, task from aws_infrastructure_security_audit_and_reporting.tools.aws_infrastructure_scanner_tool import AWSInfrastructureScannerTool from crewai_tools import SerperDevTool, ScrapeWebsiteTool import os @CrewBase class AwsInfrastructureSecurityAuditAndReportingCrew(): \u0026#34;\u0026#34;\u0026#34;AwsInfrastructureSecurityAuditAndReporting crew\u0026#34;\u0026#34;\u0026#34; def __init__(self) -\u0026gt; None: self.llm = LLM( model=os.getenv(\u0026#39;MODEL\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;), aws_region_name=os.getenv(\u0026#39;AWS_REGION_NAME\u0026#39;) ) Step 2 ‚Äì Define Agents\n@agent def infrastructure_mapper(self) -\u0026gt; Agent: return Agent( config=self.agents_config[\u0026#39;infrastructure_mapper\u0026#39;], tools=[AWSInfrastructureScannerTool()], llm=self.llm ) @agent def security_analyst(self) -\u0026gt; Agent: return Agent( config=self.agents_config[\u0026#39;security_analyst\u0026#39;], tools=[SerperDevTool(), ScrapeWebsiteTool()], llm=self.llm ) @agent def report_writer(self) -\u0026gt; Agent: return Agent( config=self.agents_config[\u0026#39;report_writer\u0026#39;], llm=self.llm ) Step 3 ‚Äì Define Tasks\n@task def map_aws_infrastructure_task(self) -\u0026gt; Task: return Task( config=self.tasks_config[\u0026#39;map_aws_infrastructure_task\u0026#39;] ) @task def exploratory_security_analysis_task(self) -\u0026gt; Task: return Task( config=self.tasks_config[\u0026#39;exploratory_security_analysis_task\u0026#39;] ) @task def generate_report_task(self) -\u0026gt; Task: return Task( config=self.tasks_config[\u0026#39;generate_report_task\u0026#39;] ) Step 4 ‚Äì Create AWS Infrastructure Scanner Tool\nclass AWSInfrastructureScannerTool(BaseTool): name: str = \u0026#34;AWS Infrastructure Scanner\u0026#34; description: str = ( \u0026#34;A tool for scanning and mapping AWS infrastructure components and their configurations. \u0026#34; \u0026#34;Can retrieve detailed information about EC2 instances, S3 buckets, IAM configurations, \u0026#34; \u0026#34;RDS instances, VPC settings, and security groups. Use this tool to gather information \u0026#34; \u0026#34;about specific AWS services or get a complete infrastructure overview.\u0026#34; ) args_schema: Type[BaseModel] = AWSInfrastructureScannerInput def _run(self, service: str, region: str) -\u0026gt; str: try: if service.lower() == \u0026#39;all\u0026#39;: return json.dumps(self._scan_all_services(region), indent=2, cls=DateTimeEncoder) return json.dumps(self._scan_service(service.lower(), region), indent=2, cls=DateTimeEncoder) except Exception as e: return f\u0026#34;Error scanning AWS infrastructure: {str(e)}\u0026#34; def _scan_all_services(self, region: str) -\u0026gt; Dict: return { \u0026#39;ec2\u0026#39;: self._scan_service(\u0026#39;ec2\u0026#39;, region), \u0026#39;s3\u0026#39;: self._scan_service(\u0026#39;s3\u0026#39;, region), \u0026#39;iam\u0026#39;: self._scan_service(\u0026#39;iam\u0026#39;, region), \u0026#39;rds\u0026#39;: self._scan_service(\u0026#39;rds\u0026#39;, region), \u0026#39;vpc\u0026#39;: self._scan_service(\u0026#39;vpc\u0026#39;, region) } Step 5 ‚Äì Assemble the Crew\n@crew def crew(self) -\u0026gt; Crew: \u0026#34;\u0026#34;\u0026#34;Creates the AwsInfrastructureSecurityAuditAndReporting crew\u0026#34;\u0026#34;\u0026#34; return Crew( agents=self.agents, tasks=self.tasks, process=Process.sequential, verbose=True, ) Step 6 ‚Äì Run the Crew\ndef run(): \u0026#34;\u0026#34;\u0026#34; Run the crew. \u0026#34;\u0026#34;\u0026#34; inputs = {} AwsInfrastructureSecurityAuditAndReportingCrew().crew().kickoff(inputs=inputs) The system generates a structured report with an executive summary, risk matrix, and prioritized remediation roadmap.\n11. Conclusion This post demonstrates how to:\nUse CrewAI and Amazon Bedrock to build sophisticated, automated security assessment systems for AWS infrastructure Design specialized agents that collaborate on mapping infrastructure, analyzing risks, and generating reports Leverage Bedrock‚Äôs managed models, Agents, and Knowledge Bases for robust, scalable agentic applications The same approach can be extended to many other domains such as process automation, research, and decision support.\n12. About the Authors Tony Kipkemboi ‚Äì Senior Developer Advocate and Partnerships Lead at CrewAI, US Army veteran with background in healthcare, data engineering, and AI. Jo√£o (Joe) Moura ‚Äì Founder and CEO of CrewAI, leading agent orchestration platform used widely across Fortune 500 companies. Karan Singh ‚Äì Generative AI Specialist at AWS, focusing on go‚Äëto‚Äëmarket strategies with foundation model and agentic framework providers. Aris Tsakpinis ‚Äì Specialist Solutions Architect for Generative AI at AWS, focusing on open source models on Amazon Bedrock and pursuing a PhD in Machine Learning Engineering. "},{"uri":"https://hainam1007.github.io/aws-ojt-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The content below is my own summary and re‚Äëframing of the AWS blog\n‚ÄúAccelerate your AWS migration with AWS Training and Certification‚Äù.\nIt is for reference only ‚Äì please do not copy verbatim for your report, including this warning.\nAccelerate your AWS migration with AWS Training and Certification Based on the blog by Srividhya Pallay, Chloe Gorgen, and Justin Lin (09 APR 2025).\n1. Why migration needs more than just technology Moving workloads to AWS Cloud is a chance to:\nImprove performance and resilience Reduce infrastructure costs Enable long‚Äëterm innovation But many migrations fail or stall not because of tools, but because of:\nMisalignment among senior leaders Vague or shifting migration goals Teams that are unfamiliar with operating on the cloud Organizations that migrate successfully tend to have:\nUnified senior leadership with clear direction and expectations. Concrete migration goals, timelines, and next steps. Top‚Äëdown investment in training, so people are confident working in the cloud. AWS Training and Certification provides targeted learning paths for each phase of a migration to build that confidence.\n2. Assess phase ‚Äì understanding where you are All migrations start with a deep look at the current IT portfolio:\nApplications and workloads Data stores and integrations Business goals, technical constraints, and current costs During this phase, organizations must:\nChoose appropriate migration strategies (from lift‚Äëand‚Äëshift to full refactor). Identify pain points and risks. Estimate cost and potential savings in the cloud. 2.1 Typical challenges Limited awareness of the 200+ AWS services available for the target architecture. Lack of agreement on the overall migration approach. Poor visibility into dependencies between legacy systems. 2.2 Helpful training resources AWS Technical Essentials\nIntroductory course for technical teams. Covers core AWS services and architectures to help envision the target state. AWS Cloud Essentials for Business Leaders\nFree course for executives, IT leaders, and LOB managers. Builds a common understanding of cloud value, risks, and migration options. With both technical and business‚Äëoriented training, organizations can perform a more thorough and realistic assessment.\n3. Mobilize phase ‚Äì getting ready to move Once the assessment is complete, the focus shifts to preparing people and environments for migration.\n3.1 Key activities Form a core migration team to coordinate efforts. Build a comprehensive migration plan with milestones and deliverables. Set up the AWS landing zone with secure networking, identity, and governance. Run pilot migrations to validate tools, processes, and assumptions. 3.2 Common roadblocks Limited hands‚Äëon experience with AWS migration tools. Uncertainty about how to configure a secure and scalable cloud environment. 3.3 Training to support mobilization AWS Migration Essentials\nTeaches how to discover, plan, execute, and track migrations\nusing services like AWS Application Migration Service, AWS DMS, and AWS Migration Hub. Designed for stakeholders across many roles. Migration Foundations Knowledge Badge Readiness Path\nLearning path for technical leaders. Combines courses, knowledge checks, two self‚Äëpaced labs, and an assessment to earn a badge. Focuses on simplifying and accelerating migrations using programs such as MAP and tools like Migration Evaluator. 4. Migrate and modernize phase ‚Äì moving and improving workloads In this phase, organizations actually move applications, workloads, and data.\n4.1 Focus areas Use insights from pilot projects to scale up migrations. Refine architectures to take advantage of: Elastic scaling Managed databases Serverless services Continuously monitor: Performance and availability Cost and resource utilization Security posture and compliance Migration is not a one‚Äëtime event. After initial cut‚Äëover, teams can start modernizing:\nBreaking monoliths into microservices Adopting serverless patterns Tuning infrastructure to fully leverage the cloud model 4.2 Training for this phase Approach to Mainframe Migration and Modernization\nFree course introducing the value and methodology of moving mainframe workloads to AWS. Designed for a broad range of roles and experience levels. Getting Started with AWS Mainframe Modernization Service\nFree course for mainframe practitioners and administrators. Explains how to use AWS Mainframe Modernization to migrate and modernize critical applications. 5. Next steps ‚Äì building a culture of continuous learning To sustain success after migration, organizations need an ongoing learning strategy:\nKeep engineering teams up‚Äëto‚Äëdate with new AWS capabilities. Strengthen skills for operating, optimizing, and evolving cloud workloads. Extend cloud literacy to finance, sales, and leadership. Recommended resources include:\nAWS Certified Cloud Practitioner\nEntry‚Äëlevel certification that validates foundational AWS cloud concepts\nfor technical and non‚Äëtechnical roles. AWS Skill Builder\nOnline training platform with 600+ free courses and learning plans. Team subscriptions help centralize learning programs for entire departments. Organizations can also connect with an AWS sales specialist to:\nPlan migration roadmaps and target architectures. Learn about programs like Migration Acceleration Program (MAP) and funding options. By combining structured training with the right programs and tools, teams can move from experimentation to a confident, well‚Äëgoverned cloud transformation.\n6. About the authors Chloe Gorgen ‚Äì Enterprise Solutions Architect at AWS, advising customers on security, analytics, data management, and automation. Passionate about youth engagement in cloud technologies.\nJustin Lin ‚Äì Small \u0026amp; Medium Business Solutions Architect at AWS, focusing on solutions that apply generative AI, natural language processing, and business intelligence.\nSrividhya Pallay ‚Äì Solutions Architect II at AWS based in Seattle, supporting small and medium‚Äësized businesses with a focus on generative AI and gaming workloads.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The content below is my own summary and re‚Äëframing of the AWS blog\n‚ÄúReserve your seat: Microsoft workloads on AWS sessions at re:Invent 2024‚Äù.\nIt is for reference only ‚Äì please do not copy verbatim for your report, including this warning.\nReserve your seat: Microsoft workloads on AWS sessions at re:Invent 2024 Based on the blog by David Pallmann (04 NOV 2024).\n1. re:Invent 2024 and Microsoft workloads on AWS AWS re:Invent 2024 is approaching fast, with more than 2,400 sessions spread across six venues.\nRegistered attendees can already log in to the Attendee Portal and reserve seats, which is strongly recommended because popular sessions fill up quickly.\nThis post highlights sessions focused on Microsoft workloads on AWS, including:\nWindows Server SQL Server Active Directory .NET and application modernization The goal is to help you quickly find the content that will improve how you migrate, optimize, and modernize Microsoft-based environments on AWS.\n2. Breakout sessions Breakout sessions are 45‚Äì60 minute lecture-style talks led by AWS experts, customers, or partners, usually with time for Q\u0026amp;A.\nXNT202 ‚Äì The art and culture of modernizing a .NET monolith Story of Tessitura, an arts-and-culture software provider, transforming a 20‚Äëyear‚Äëold client‚Äëserver .NET system into a modern cloud architecture. Shows how AWS‚Äôs Experience-Based Acceleration (EBA) helped drive both technical change and organizational culture shifts, and shares lessons for anyone modernizing legacy .NET. XNT203 ‚Äì Supercharge your SQL Server workloads, featuring Thomson Reuters Explains how Thomson Reuters re‚Äëarchitected its SQL Server environment on AWS for a mission‚Äëcritical trade management platform. Attendees learn how they achieved ~40% lower operating cost and 5√ó operational efficiency while preserving both managed-platform convenience and OS‚Äëlevel control. XNT204 ‚Äì Licensing commercial software on AWS Clarifies licensing paths for Microsoft software and other commercial platforms on AWS. Covers AWS Optimization and Licensing Assessment (AWS OLA), end‚Äëof‚Äësupport scenarios, VMware cost considerations, and ways to replace traditional licensed products with cloud‚Äënative or open-source options. XNT302 ‚Äì Harness Amazon Q Business power with Microsoft workload data sources Shows how data in SharePoint and Windows file servers (via Amazon FSx) becomes a rich source for Amazon Q Business. Demonstrates indexing, enforcing guardrails with Active Directory identities, and using generative AI securely over Microsoft content. XNT303 ‚Äì Reduce cost for SQL Server workloads on AWS using Optimize CPU feature Focuses on using Optimize CPU settings on Amazon EC2 to tune vCPU allocation for SQL Server workloads that are more memory/IO‚Äëbound than CPU‚Äëbound. Shares test results (including HammerDB benchmarks) and best practices to cut cost while maintaining performance. XNT311 ‚Äì Go codeless and replatform Windows applications on Amazon EKS Moody‚Äôs discusses moving dozens of .NET apps from EC2 for Windows Server to Amazon EKS with Windows support. Attendees see patterns, challenges, and platform‚Äëengineering practices that enabled cost savings, higher performance, and more flexibility. XNT312 ‚Äì Use generative AI to optimize cloud operations for Microsoft workloads Demonstrates how Amazon Q and Amazon Bedrock can simplify operations for Windows-based infrastructure. Examples include natural‚Äëlanguage driven log analysis, configuration drift detection, and automation script generation. XNT314 ‚Äì Unleashing .NET workloads: From data center to multi-region resilience Verisk Analytics shares its eight‚Äëyear modernization journey taking .NET workloads from data center to multi‚ÄëRegion, highly available deployments on AWS. Session covers cost controls (including warm pools) and lessons learned around server sprawl and Microsoft licensing. XNT313 ‚Äì Migrate and modernize a Microsoft-based banking solution with AWS Targets financial institutions moving Microsoft‚Äëbased stacks to AWS to enable AI/ML adoption and better risk management. Reviews migration, optimization, and modernization steps for Windows Server, SQL Server, .NET, and Active Directory workloads. XNT315 ‚Äì Modernizing a spin-off in the cloud: Allspring‚Äôs journey with AWS Describes how Allspring Global Investments, spun out of Wells Fargo, moved off on‚Äëpremises infrastructure under tight timelines. Shows how they used Amazon EC2, Amazon EBS, and Amazon RDS for SQL Server and Oracle as a foundation, and are now modernizing further with cloud‚Äënative and open-source services. XNT316 ‚Äì From Azure to AWS: One organization‚Äôs all-in journey with AWS Bonterra Inc. explains why it chose AWS as its strategic cloud over Azure. Covers the rapid migration, architectural choices, and how the company co‚Äëinnovates with AWS to deliver solutions for the social good sector. XNT317 ‚Äì How Mindbody migrated 60,000 SQL Server databases to AWS Mindbody recounts migrating a large monolithic Windows/SQL Server application with over 60,000 databases to AWS Local Zones and Regions. Discusses latency challenges, architecture patterns, licensing and cost decisions, and the operational improvements gained. 3. Builders‚Äô sessions Builders‚Äô sessions are small, hands‚Äëon labs (10 attendees per expert) with no slides‚Äîjust laptops and guided building.\nXNT307 ‚Äì Unlock your data: Build AI applications with .NET and Amazon Bedrock Hands‚Äëon implementation of Retrieval Augmented Generation (RAG) using a .NET app and Amazon Bedrock Knowledge Bases. Participants wire up private data sources so generative AI applications can answer questions grounded in enterprise content. XNT318 ‚Äì Boost .NET developer productivity with Amazon Q Developer Shows how Amazon Q Developer accelerates .NET development through inline code suggestions and automation. Attendees practice generating boilerplate code, iterating on prototypes, understanding unfamiliar code, and improving quality and security. 4. Chalk talks Chalk talks are interactive whiteboard sessions with a short intro and then open Q\u0026amp;A.\nXNT201 ‚Äì AWS fundamentals for the accidental SQL Server database administrator Oriented to DBAs who suddenly find themselves responsible for AWS environments. Maps familiar SQL Server admin tasks to cloud architecture, discusses networking, security, HA/DR, and how generative AI (Amazon Q Business / Developer) can assist. XNT304 ‚Äì How to navigate your SQL Server migration journey with AWS Reviews the spectrum of SQL Server migration strategies on AWS. Compares AWS Migration Hub, Application Migration Service, AWS DMS, and native SQL Server tools such as replication and Always On, backed by lessons from real projects. 5. Code talks Code talks are demo‚Äëheavy, code‚Äëfocused sessions with a small audience and live examples.\nXNT305 ‚Äì Choose the right AI service for your .NET use case Walks through when to use different AWS AI and generative AI services from .NET via the AWS SDK. Includes building an intelligent document processing sample that uses both traditional AI and generative AI features. XNT309 ‚Äì Automate and troubleshoot AD using Amazon Bedrock Shows how Bedrock Agents can automate routine Active Directory tasks and assist with troubleshooting. Demos include analyzing logs and events to diagnose account lockouts, Kerberos issues, and providing natural‚Äëlanguage remediation steps. XNT310 ‚Äì Accelerate .NET development with Amazon Q Developer Explores using Amazon Q Developer across multiple IDEs for writing, testing, debugging, and upgrading .NET code faster. Covers privacy and enterprise controls, plus examples of scanning for issues and answering deep technical questions. XNT402 ‚Äì Build an image search engine with .NET and Amazon Bedrock Guides attendees through building a semantic image search solution using Amazon Bedrock and Amazon OpenSearch Service. Includes preprocessing images into embeddings, indexing them, and creating a .NET front end for search. 6. Workshops Workshops are two‚Äëhour group labs where attendees collaboratively build full solutions with AWS experts.\nXNT306 ‚Äì From diagram to code: Automate IaC code generation with Amazon Bedrock Participants build an end‚Äëto‚Äëend .NET 8 app using Amazon Bedrock, AWS Lambda, and Amazon API Gateway. The focus is generating infrastructure‚Äëas‚Äëcode directly from architecture diagrams and then using agents to provision infrastructure securely. XNT308 ‚Äì Modernize Windows applications on your own terms Attendees analyze several Windows applications and choose modernization paths such as containerizing to Amazon ECS/EKS or refactoring .NET to run on Linux. Emphasizes thinking through dependencies and picking the right modernization strategy per app. XNT401 ‚Äì Easily port .NET Framework applications to Linux with Amazon Q Hands‚Äëon use of Amazon Q Code Transformation for .NET to move .NET Framework apps from Windows Server to cross‚Äëplatform .NET on Linux. Highlights how this reduces licensing costs and can improve performance and security. XNT403 ‚Äì Securing AWS Managed Microsoft AD the AWS Well-Architected way Applies the AWS Well‚ÄëArchitected Framework ‚Äì security pillar to AWS Managed Microsoft AD. Uses IAM, CloudTrail, VPC configurations, and other services to build defense‚Äëin‚Äëdepth and align with security best practices. XNT404 ‚Äì Migrate to an open source database on AWS using .NET and PostgreSQL Demonstrates moving a .NET application from SQL Server to Amazon RDS for PostgreSQL using AWS DMS. Discusses code and schema refactoring, common migration challenges, and repeatable techniques to adopt open-source databases. 7. Join us in Las Vegas The blog closes by encouraging readers to:\nRegister for re:Invent (if they haven‚Äôt already) and reserve seats for key sessions via the Attendee Portal. Visit the Windows, SQL Server, and .NET kiosk in the AWS Village expo area to: Meet experts building Microsoft‚Äërelated services See demos, including generative AI scenarios Ask questions and dive deeper into architectures for Microsoft workloads on AWS The overall message: AWS offers a broad set of services and features to run and modernize Microsoft applications efficiently, and re:Invent is the ideal place to learn how.\n8. About the author David Pallmann is a senior product manager on the AWS Transform team, focusing on the .NET developer experience.\nHe has held roles in engineering, consulting, product management, and technical leadership; previously worked on Windows Communication Foundation (WCF) and later created Neuron ESB, a .NET‚Äëbased enterprise service bus.\nYou can follow him on X at @davidpallmann.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.6-cleanup/5.6.1-destroy-stack/","title":"Destroy the Stack","tags":[],"description":"","content":"The beauty of Infrastructure as Code (IaC) with AWS CDK is that you can dismantle the entire system with a single command.\nCDK knows exactly what it created and will delete resources in the correct dependency order.\nNavigate to CDK Directory Make sure you are in the cdk directory:\ncd cdk Run Destroy Command Execute the following command:\ncdk destroy Confirmation Prompt The CLI will ask for confirmation:\nAre you sure you want to delete: BackendStack (y/n)? Type y and press Enter.\nDeletion Process The deletion process generally takes 3-5 minutes.\nYou will see output similar to:\nBackendStack: destroying... ‚úÖ BackendStack: destroyed Stack ARN: arn:aws:cloudformation:us-east-1:123456789012:stack/BackendStack/... What Gets Deleted CDK will automatically delete all resources in the correct order:\nAPI Gateway - REST API and deployment Lambda Function - Including associated IAM roles DynamoDB Tables - All 7 tables Cognito User Pool - Users and groups Cognito Identity Pool - Identity mappings S3 Bucket - Including all objects (due to autoDeleteObjects: true) Location Service - Maps, Place Index, Route Calculator CloudWatch Log Groups - Lambda execution logs IAM Roles and Policies - All service permissions CloudFormation Stack - The stack itself The RemovalPolicy.DESTROY configuration in our CDK code ensures that data-containing resources (like DynamoDB tables and S3 buckets) are also deleted.\nMonitor Deletion Progress You can monitor the deletion progress in:\nTerminal: Watch the CLI output AWS Console: Go to CloudFormation ‚Üí Stacks ‚Üí BackendStack ‚Üí Events tab Troubleshooting Deletion Issues If the deletion fails:\nIssue: S3 Bucket Not Empty\n# Manually empty the bucket aws s3 rm s3://findnest-images-YOUR-ACCOUNT-ID --recursive # Then retry destroy cdk destroy Issue: DynamoDB Table Has Deletion Protection\nIf you modified the tables manually in console and enabled deletion protection, you need to disable it first:\nGo to DynamoDB Console Select the table Go to Additional settings tab Turn off Deletion protection Retry cdk destroy Issue: Lambda Execution Role In Use\nWait a few minutes for Lambda executions to complete, then retry.\nOnce you run cdk destroy and confirm, the process cannot be undone. All data will be permanently deleted.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/4-eventparticipated/4.1-event1/","title":"DevOps on AWS","tags":[],"description":"","content":"Summary Report: \u0026ldquo;DevOps on AWS Training Session\u0026rdquo; Event Information Event: DevOps on AWS Training Session\nDate: Monday, November 17, 2025\nTime: 8:30 AM ‚Äì 5:00 PM\nLocation: AWS Office\nDuration: Full-day intensive training (8.5 hours)\nTraining Overview On November 17, 2025, I attended a comprehensive full-day DevOps on AWS Training Session at the AWS Office. This intensive training provided hands-on experience with AWS DevOps services, covering everything from CI/CD pipelines to container orchestration and monitoring. The session built upon the AI/ML knowledge from the previous workshop, showing how to operationalize and automate deployments at scale.\nSession Breakdown 1. Welcome \u0026amp; DevOps Mindset The training began by establishing the foundational concepts of DevOps culture and principles.\nDevOps Culture \u0026amp; Principles The instructor emphasized that DevOps is not just about tools‚Äîit\u0026rsquo;s a cultural transformation:\nCollaboration: Breaking down silos between development and operations Automation: Reducing manual work and human error Continuous Improvement: Learning from failures and iterating Shared Responsibility: Everyone owns the entire lifecycle Key DevOps Metrics (DORA) We learned about the four key metrics from Google\u0026rsquo;s DevOps Research and Assessment (DORA):\nMetric Elite Performers Our Current State Target Deployment Frequency Multiple times per day Weekly Daily Lead Time for Changes Less than 1 hour 2-3 days \u0026lt; 1 day Mean Time to Recovery (MTTR) Less than 1 hour 4-6 hours \u0026lt; 2 hours Change Failure Rate 0-15% 20-25% \u0026lt; 15% Key Insight: Understanding these metrics helps quantify DevOps transformation success and identify areas for improvement.\nConnection to AI/ML Workshop The instructor briefly recapped the previous AI/ML session, showing how DevOps practices enable rapid deployment of AI models and continuous improvement through MLOps.\n2. AWS DevOps Services ‚Äì CI/CD Pipeline This section provided hands-on experience building automated CI/CD pipelines using AWS native services.\nSource Control with AWS CodeCommit What is CodeCommit?\nFully managed Git-based source control service Secure, scalable, and integrated with AWS services No server management required Git Branching Strategies:\nGitFlow Model:\nmain (production) ‚Üì develop (integration) ‚Üì feature/*, hotfix/*, release/* Trunk-Based Development:\nmain (always deployable) ‚Üì short-lived feature branches (1-2 days max) Our Discussion: The trainer recommended trunk-based development for teams practicing continuous deployment, while GitFlow works better for scheduled releases.\nBuild \u0026amp; Test with AWS CodeBuild CodeBuild Overview:\nFully managed build service Pay only for build time used Supports Docker, custom build environments Integrates with CodePipeline Buildspec.yml Example:\nversion: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_REPO build: commands: - echo Build started on `date` - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG . - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $ECR_REPO/$IMAGE_REPO_NAME:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - docker push $ECR_REPO/$IMAGE_REPO_NAME:$IMAGE_TAG Testing Integration:\nUnit tests run during build phase Integration tests in separate stage Security scanning with CodeGuru Code coverage reports to CloudWatch Deployment with AWS CodeDeploy CodeDeploy Deployment Strategies:\nStrategy Description Use Case Rollback Time All-at-Once Deploy to all instances simultaneously Dev/test environments Immediate Rolling Deploy in batches Production with minimal downtime Minutes Blue/Green New environment, switch traffic Zero-downtime production Seconds Canary Gradual traffic shift (10% ‚Üí 50% ‚Üí 100%) High-risk changes Phased Blue/Green Deployment Demo:\nBlue Environment (Current Production) ‚Üì Green Environment (New Version) deployed ‚Üì Traffic shifted from Blue ‚Üí Green ‚Üì Blue kept as instant rollback option Key Advantage: Instant rollback by redirecting traffic back to the blue environment if issues arise.\nOrchestration with AWS CodePipeline CodePipeline Architecture:\nSource (CodeCommit) ‚Üí Build (CodeBuild) ‚Üí Test ‚Üí Deploy (CodeDeploy) ‚Üí Production ‚Üì Manual Approval (optional) Pipeline Stages We Built:\nSource: Triggered by Git commit Build: Compile code, run unit tests Test: Integration tests, security scans Staging Deploy: Deploy to staging environment Manual Approval: Product owner review Production Deploy: Blue/green deployment Post-Deploy Tests: Smoke tests, health checks Demo Walkthrough: The trainer demonstrated a complete pipeline deploying a Node.js application:\nCommit code to CodeCommit Automatic build triggered Tests pass, artifact created Manual approval requested Blue/green deployment to production Automatic rollback on health check failure This was incredibly practical‚Äîseeing the entire flow from commit to production in under 10 minutes was eye-opening.\n3. Infrastructure as Code (IaC) AWS CloudFormation What is CloudFormation? Service for modeling and provisioning AWS resources using templates.\nCloudFormation Template Structure:\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: Web application infrastructure Parameters: InstanceType: Type: String Default: t3.micro AllowedValues: - t3.micro - t3.small Resources: WebServerInstance: Type: AWS::EC2::Instance Properties: InstanceType: !Ref InstanceType ImageId: ami-0c55b159cbfafe1f0 LoadBalancer: Type: AWS::ElasticLoadBalancingV2::LoadBalancer Properties: Subnets: - subnet-12345 - subnet-67890 Outputs: WebsiteURL: Value: !GetAtt LoadBalancer.DNSName Key Concepts:\nStacks: Collection of AWS resources managed as a single unit Change Sets: Preview changes before applying Drift Detection: Identify manual changes to resources Stack Policies: Prevent accidental updates to critical resources AWS CDK (Cloud Development Kit) What is CDK? Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#).\nCDK vs CloudFormation:\nAspect CloudFormation CDK Syntax YAML/JSON Python, TypeScript, etc. Abstraction Low-level High-level constructs Reusability Limited Highly reusable patterns Complexity Verbose for complex setups Simplified with logic CDK Example (TypeScript):\nimport * as cdk from \u0026#39;aws-cdk-lib\u0026#39;; import * as ec2 from \u0026#39;aws-cdk-lib/aws-ec2\u0026#39;; import * as ecs from \u0026#39;aws-cdk-lib/aws-ecs\u0026#39;; import * as elbv2 from \u0026#39;aws-cdk-lib/aws-elasticloadbalancingv2\u0026#39;; export class MyAppStack extends cdk.Stack { constructor(scope: cdk.App, id: string, props?: cdk.StackProps) { super(scope, id, props); // Create VPC const vpc = new ec2.Vpc(this, \u0026#39;MyVPC\u0026#39;, { maxAzs: 2 }); // Create ECS Cluster const cluster = new ecs.Cluster(this, \u0026#39;MyCluster\u0026#39;, { vpc: vpc }); // Create Load Balancer const lb = new elbv2.ApplicationLoadBalancer(this, \u0026#39;LB\u0026#39;, { vpc, internetFacing: true }); // Output load balancer URL new cdk.CfnOutput(this, \u0026#39;LoadBalancerDNS\u0026#39;, { value: lb.loadBalancerDnsName }); } } Benefits of CDK:\nUse programming language features (loops, conditionals, functions) Better IDE support (autocomplete, type checking) Easier testing with unit tests Reusable constructs and patterns Demo: Deploying with Both Tools\nThe trainer deployed the same infrastructure using:\nCloudFormation template (150 lines of YAML) CDK code (50 lines of TypeScript) Both created identical AWS resources, but CDK was significantly more readable and maintainable.\nChoosing Between IaC Tools Decision Framework:\nScenario Recommended Tool Reason Simple infrastructure CloudFormation Direct AWS support, no dependencies Complex multi-service apps CDK Code reusability, better abstraction Multi-cloud deployments Terraform Cloud-agnostic approach Team already using Ansible Ansible Leverage existing expertise 4. Container Services on AWS Docker Fundamentals Why Containers?\nConsistency: \u0026ldquo;Works on my machine\u0026rdquo; ‚Üí \u0026ldquo;Works everywhere\u0026rdquo; Isolation: Each service has its own dependencies Portability: Run anywhere Docker is available Efficiency: Share OS kernel, faster than VMs Microservices Architecture:\nMonolith ‚Üí Microservices Single app ‚Üí Order Service + Payment Service + Inventory Service + Shipping Service Amazon ECR (Elastic Container Registry) What is ECR? Fully managed Docker container registry for storing, managing, and deploying container images.\nKey Features:\nImage Scanning: Automatic vulnerability detection Lifecycle Policies: Auto-delete old images to save costs Encryption: Images encrypted at rest IAM Integration: Fine-grained access control ECR Workflow:\n# Authenticate Docker to ECR aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_URI # Tag image docker tag my-app:latest $ECR_URI/my-app:latest # Push to ECR docker push $ECR_URI/my-app:latest Amazon ECS (Elastic Container Service) ECS Overview: Fully managed container orchestration service.\nECS Concepts:\nTask Definition: Blueprint for your application (Docker image, CPU, memory) Service: Maintains desired number of tasks running Cluster: Logical grouping of tasks or services ECS Launch Types:\nLaunch Type Infrastructure Use Case Management EC2 You manage EC2 instances Cost optimization, custom configs More control Fargate AWS manages servers Simplicity, focus on apps Less management Task Definition Example:\n{ \u0026#34;family\u0026#34;: \u0026#34;web-app\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;123456789.dkr.ecr.us-east-1.amazonaws.com/web-app:latest\u0026#34;, \u0026#34;memory\u0026#34;: 512, \u0026#34;cpu\u0026#34;: 256, \u0026#34;portMappings\u0026#34;: [{ \u0026#34;containerPort\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34; }] }] } Amazon EKS (Elastic Kubernetes Service) When to Use EKS?\nAlready using Kubernetes on-premises Need Kubernetes-specific features Multi-cloud strategy with K8s Large-scale, complex orchestration needs ECS vs EKS:\nFactor ECS EKS Complexity Simpler More complex AWS Integration Deep Standard K8s Ecosystem AWS-specific Kubernetes ecosystem Cost Lower (no control plane fee) Higher (control plane ~$73/month) Trainer\u0026rsquo;s Recommendation: Start with ECS unless you specifically need Kubernetes features or have existing K8s expertise.\nAWS App Runner What is App Runner? Fully managed service that builds and deploys containerized web applications automatically.\nApp Runner Benefits:\nNo infrastructure management Automatic scaling (0 to N) Built-in load balancing and TLS Pay only for running time When to Use App Runner:\nSimple web applications or APIs Don\u0026rsquo;t need advanced container orchestration Want the fastest path to production Comparison:\nComplexity: EC2 \u0026gt; ECS \u0026gt; EKS \u0026gt; App Runner Control: EC2 \u0026gt; ECS \u0026gt; EKS \u0026gt; App Runner Simplicity: App Runner \u0026gt; EKS \u0026gt; ECS \u0026gt; EC2 Demo \u0026amp; Case Study: Microservices Deployment The trainer demonstrated deploying a three-tier application:\nArchitecture:\nALB ‚Üí ECS Service (Frontend) ‚Üí ECS Service (API) ‚Üí RDS Database Deployment Steps:\nBuild Docker images for frontend and API Push images to ECR Create ECS task definitions Deploy services with Fargate Configure ALB to route traffic Set up auto-scaling policies Results:\nDeployment completed in 15 minutes Automatic scaling from 2 to 10 tasks under load Zero-downtime updates with rolling deployments 5. Monitoring \u0026amp; Observability Amazon CloudWatch CloudWatch Services:\n1. Metrics\nPre-built metrics (CPU, memory, network) Custom application metrics Metric math for calculations 2. Logs\nCentralized log aggregation Log insights for querying Log groups and retention policies 3. Alarms\nThreshold-based alerts Anomaly detection Composite alarms (multiple conditions) 4. Dashboards\nVisual representation of metrics Custom widgets and graphs Shareable for team visibility CloudWatch Alarm Example:\nType: AWS::CloudWatch::Alarm Properties: AlarmName: HighCPUUtilization MetricName: CPUUtilization Namespace: AWS/EC2 Statistic: Average Period: 300 EvaluationPeriods: 2 Threshold: 80 ComparisonOperator: GreaterThanThreshold AlarmActions: - !Ref SNSTopic AWS X-Ray What is X-Ray? Distributed tracing service for analyzing and debugging microservices applications.\nX-Ray Capabilities:\nService Map: Visual representation of application components Trace Analysis: Follow requests through services Performance Insights: Identify bottlenecks Error Detection: Trace failed requests How X-Ray Works:\nRequest ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB ‚Üì ‚Üì ‚Üì ‚Üì X-Ray records timing and metadata at each step ‚Üì Service map shows complete request flow Use Case Example: A user reports slow page load. With X-Ray, we traced the request and found:\nAPI Gateway: 50ms Lambda function: 2000ms (bottleneck!) Database query: 1800ms (slow query) Business logic: 200ms DynamoDB: 100ms Solution: Optimized the database query, reducing Lambda time from 2000ms to 300ms.\nDemo: Full-Stack Observability Setup The trainer demonstrated setting up comprehensive monitoring:\n1. Application Metrics\n# Send custom metric to CloudWatch import boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) cloudwatch.put_metric_data( Namespace=\u0026#39;MyApp\u0026#39;, MetricData=[{ \u0026#39;MetricName\u0026#39;: \u0026#39;OrdersProcessed\u0026#39;, \u0026#39;Value\u0026#39;: 1, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; }] ) 2. Log Aggregation\nConfigured CloudWatch Logs agent on EC2 instances Set up log groups for different services Created log insights queries for error analysis 3. Distributed Tracing\nInstrumented Lambda functions with X-Ray SDK Enabled X-Ray on API Gateway Visualized complete request flows 4. Dashboards\nCreated operational dashboard showing: Request rate and latency Error rates by service Infrastructure health (CPU, memory) Custom business metrics Best Practices for Monitoring Alerting Strategy:\nCritical Alerts: Page on-call engineer immediately Warning Alerts: Notify team via Slack Informational: Log for later review Dashboard Design:\nExecutive Dashboard: High-level business metrics Operations Dashboard: System health and performance Developer Dashboard: Service-specific metrics and logs On-Call Processes:\nDefined escalation paths Runbooks for common issues Post-incident review process 6. DevOps Best Practices \u0026amp; Case Studies Deployment Strategies Feature Flags:\n// Use feature flags to control rollout if (featureFlags.isEnabled(\u0026#39;new-checkout-flow\u0026#39;, userId)) { return newCheckoutProcess(); } else { return legacyCheckoutProcess(); } Benefits:\nDecouple deployment from release Test in production with subset of users Instant rollback without redeployment A/B Testing:\nSplit traffic between versions Measure conversion rates, performance Data-driven deployment decisions Automated Testing in CI/CD Testing Pyramid:\nE2E Tests (Few) ‚Üó Integration Tests (Some) ‚Üó Unit Tests (Many) Test Coverage in Pipeline:\nUnit Tests: Every build (\u0026lt; 1 minute) Integration Tests: Every commit to main (5-10 minutes) E2E Tests: Before production deploy (15-20 minutes) Performance Tests: Scheduled nightly Incident Management Incident Response Process:\nDetection: Automated alerts or user reports Triage: Assess severity and impact Investigation: Use logs, metrics, traces Mitigation: Apply fix or rollback Communication: Update stakeholders Resolution: Verify fix and close Post-Incident Review:\nWhat happened? What was the impact? Root cause analysis What went well? What can we improve? Action items with owners Blameless Culture: Focus on systems and processes, not individuals.\nCase Studies Case Study 1: Startup Migration to AWS DevOps\nBefore: Manual deployments, 2-week release cycle After: Automated CI/CD, daily deployments Results: 85% faster time-to-market, 60% fewer production incidents Case Study 2: Enterprise DevOps Transformation\nChallenge: Legacy monolith, quarterly releases Solution: Microservices + ECS + CodePipeline Results: Weekly releases, 40% infrastructure cost reduction Lessons Learned:\nStart small with pilot projects Invest in training and culture change Measure and iterate continuously Executive sponsorship is critical 7. Q\u0026amp;A \u0026amp; Wrap-up DevOps Career Pathways The trainer outlined career progression in DevOps:\nJunior DevOps Engineer ‚Üí DevOps Engineer ‚Üí Senior DevOps/SRE ‚Üí DevOps Architect/Manager\nKey Skills to Develop:\nCloud platforms (AWS, Azure, GCP) Infrastructure as Code (Terraform, CloudFormation) Container orchestration (Docker, Kubernetes) CI/CD tools (Jenkins, GitLab CI, AWS CodePipeline) Scripting (Python, Bash) Monitoring and observability Security best practices (DevSecOps) AWS Certification Roadmap Recommended Path:\nAWS Certified Cloud Practitioner (Foundational) AWS Certified Developer ‚Äì Associate or AWS Certified SysOps Administrator ‚Äì Associate AWS Certified DevOps Engineer ‚Äì Professional Specialty Certifications (Security, Advanced Networking, etc.) Certification Benefits:\nValidate knowledge and skills Increase earning potential Stand out in job market Free practice exams and resources Key Takeaways Technical Skills Acquired ‚úÖ CI/CD Mastery: Built complete pipelines from source to production\n‚úÖ Infrastructure as Code: Can provision AWS resources using CloudFormation and CDK\n‚úÖ Container Orchestration: Understand when to use ECS, EKS, or App Runner\n‚úÖ Observability: Set up comprehensive monitoring with CloudWatch and X-Ray\n‚úÖ Best Practices: Learned deployment strategies and incident management\nStrategic Insights DevOps is Cultural, Not Just Technical\nSuccess requires organizational buy-in Break down silos between teams Embrace failure as learning opportunity Automation is Key to Scaling\nManual processes don\u0026rsquo;t scale Invest time in automation upfront Reap benefits over time Measure Everything\nDORA metrics provide objective goals Use data to drive improvements Celebrate wins and learn from failures Choose the Right Level of Abstraction\nEC2 for maximum control ECS for balance of control and simplicity Fargate/App Runner for least management Security Must Be Built In, Not Bolted On\nShift security left in pipeline Automate security scanning Implement least privilege access How I Will Apply This Knowledge Immediate Actions (This Week) Set Up CI/CD Pipeline for Current Project\nMigrate from manual deployments to CodePipeline Implement automated testing Target: First automated deployment by Friday Containerize Development Environment\nCreate Docker Compose setup for local development Standardize environment across team Eliminate \u0026ldquo;works on my machine\u0026rdquo; issues Implement Basic Monitoring\nSet up CloudWatch dashboards Configure alarms for critical metrics Document runbooks for common alerts Short-Term Goals (Next Month) Infrastructure as Code Adoption\nConvert existing manual infrastructure to CDK Implement version control for infrastructure Enable infrastructure code reviews Blue/Green Deployment\nImplement for production application Test rollback procedures Measure deployment success rate Enhanced Observability\nInstrument application with X-Ray Create distributed tracing for critical flows Build executive dashboard with business metrics Long-Term Vision (Next Quarter) Full DevOps Transformation\nAchieve daily deployment frequency Reduce MTTR to under 2 hours Lower change failure rate below 15% Container Migration\nMove microservices to ECS Fargate Implement auto-scaling policies Reduce infrastructure costs by 30% DevSecOps Integration\nAutomate security scanning in pipeline Implement compliance checks Conduct chaos engineering experiments Team Capability Building\nConduct internal DevOps workshop Create reusable pipeline templates Establish DevOps best practices guide Training Experience What Made This Training Effective Comprehensive Coverage\nThe full-day format allowed deep dives into each topic rather than surface-level overviews. By the end, I felt confident implementing what I learned.\nHands-On Approach\nEvery concept was followed by a practical demonstration. Building actual pipelines and deploying real applications made the learning concrete and applicable.\nReal-World Focus\nThe trainer shared war stories from actual projects, including failures and lessons learned. This honest perspective was invaluable.\nAWS Best Practices\nRather than just showing what\u0026rsquo;s possible, the trainer emphasized what works well in production and common pitfalls to avoid.\nTraining Environment The AWS Office provided:\nModern training room with dual monitors Sandbox AWS accounts for hands-on practice High-speed internet for demos Catered lunch and refreshments Training materials and code samples What I Appreciated Most ‚ú® Practical Demos: Every service demonstrated with working examples\n‚ú® Comparison Frameworks: Clear guidance on choosing between options\n‚ú® Case Studies: Real-world examples of DevOps transformations\n‚ú® Career Guidance: Advice on learning paths and certifications\n‚ú® Takeaway Resources: Code samples, templates, and documentation\nResources Provided Documentation \u0026amp; Guides AWS DevOps documentation links CI/CD pipeline templates IaC best practices guide Container deployment patterns Monitoring setup guides Code Samples Complete CodePipeline configurations CloudFormation and CDK templates Docker and ECS task definitions X-Ray instrumentation examples CloudWatch dashboard JSON Additional Learning AWS DevOps blog links Certification study guides Recommended books and courses Community forums and Slack channels Event Photos Full-day DevOps training session begins at AWS Office\nLive demonstration of CodePipeline deploying application\nHands-on Infrastructure as Code session with CDK\nECS Fargate deployment demonstration\nCloudWatch dashboard showing full-stack observability\nQ\u0026amp;A session with AWS DevOps experts\nTraining completion certificate\nNext Steps \u0026amp; Action Plan Week 1 Set up CodeCommit repository for current project Create basic CodeBuild buildspec.yml Configure CodePipeline with manual approval Test first automated deployment Week 2 Implement blue/green deployment strategy Set up CloudWatch dashboards Configure alarms for critical metrics Document deployment procedures Week 3-4 Containerize one microservice Deploy to ECS Fargate Implement X-Ray tracing Conduct performance testing Month 2 Convert infrastructure to CDK Establish IaC code review process Automate security scanning Present DevOps roadmap to team Quarter 1 2026 Achieve daily deployment frequency Reduce MTTR below 2 hours Complete container migration Obtain AWS DevOps Engineer certification Conclusion The DevOps on AWS Training Session was one of the most comprehensive and practical training experiences I\u0026rsquo;ve attended. The full-day format provided sufficient depth to move beyond theory and gain hands-on experience with AWS DevOps services.\nTransformation in Understanding Before Training:\nDevOps seemed like a complex, distant goal CI/CD was intimidating and unclear Infrastructure management felt manual and error-prone After Training:\nDevOps is achievable with the right tools and mindset CI/CD can be implemented incrementally Infrastructure as Code makes operations reproducible and scalable Most Valuable Learnings CI/CD is Not All-or-Nothing: Start with basic pipeline, iterate and improve Choose Simplicity When Possible: App Runner and Fargate reduce operational burden Observability is Critical: Can\u0026rsquo;t improve what you don\u0026rsquo;t measure Cultural Change Takes Time: Technical transformation must be accompanied by organizational buy-in Career Impact This training significantly advanced my DevOps capabilities. I now have:\nPractical experience with AWS DevOps services Clear understanding of modern deployment practices Confidence to lead DevOps initiatives Roadmap for professional development I\u0026rsquo;m excited to apply these skills to transform our deployment processes and work toward AWS DevOps Engineer certification.\nWould I recommend this training? Absolutely‚Äîessential for anyone working with AWS who wants to implement modern DevOps practices.\nAdditional Resources AWS Documentation AWS DevOps Blog CI/CD on AWS AWS Well-Architected Framework - Operational Excellence Learning Paths AWS DevOps Engineer - Professional Certification AWS Training and Certification Books \u0026amp; References \u0026ldquo;The Phoenix Project\u0026rdquo; by Gene Kim \u0026ldquo;Accelerate\u0026rdquo; by Nicole Forsgren, Jez Humble, Gene Kim \u0026ldquo;Site Reliability Engineering\u0026rdquo; by Google Community AWS DevOps Slack community r/devops on Reddit Local AWS user groups Training attended: November 17, 2025 | AWS Office\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Serverless Architecture Serverless architecture allows you to build and run applications and services without thinking about servers. It eliminates infrastructure management tasks such as server provisioning, patching, operating system maintenance, and capacity provisioning. The architecture relies on AWS Lambda for compute, Amazon API Gateway for API management, and Amazon DynamoDB for data storage. Compute resources running in Lambda are triggered by events from API Gateway and scale automatically.\nWorkshop overview In this workshop, you will deploy the Findnest Backend using AWS CDK. \u0026ldquo;Backend Logic\u0026rdquo; is handled by AWS Lambda running Node.js/Express code. It serves as the \u0026ldquo;brain\u0026rdquo; of the application, processing business logic only when triggered. \u0026ldquo;Data \u0026amp; Storage\u0026rdquo; utilizes Amazon DynamoDB for high-performance NoSQL data storage (Listings, Users) and Amazon S3 for storing user-uploaded images securely. Amazon API Gateway acts as the secure entry point for all client requests.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/","title":"OJT Report","tags":[],"description":"","content":"Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguy·ªÖn H·∫£i Nam\nPhone Number: 0986510235\nEmail: hainambl996@gmail.com\nUniversity: FPT University Ho Chi Minh\nMajor: Software Engineering\nClass: FCJ - FPTU\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/5.5.1-retrieve-config/","title":"Retrieve Configuration","tags":[],"description":"","content":"Ensure you have the ApiUrl from the outputs of Section 5.3 (Deploy Backend).\nLocate the API URL After running cdk deploy, you should have received outputs similar to:\n‚úÖ BackendStack Outputs: BackendStack.ApiUrl = https://xyz123.execute-api.us-east-1.amazonaws.com/prod/ BackendStack.UserPoolId = us-east-1_AbCdEfGh ... API URL Format The API URL follows this format:\nhttps://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/prod/ Example:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/ The /prod/ at the end indicates the deployment stage. This is where your API Gateway routes all incoming requests.\nSave the API URL Copy and save this URL. You will use it as the base URL for all API requests in the following test cases.\nIf you lost the outputs, you can retrieve them by:\nRunning cdk deploy again (it won\u0026rsquo;t redeploy if nothing changed) Checking the CloudFormation stack outputs in AWS Console Going to API Gateway Console and finding your API\u0026rsquo;s invoke URL Verify API Gateway You can quickly verify that API Gateway is accessible by opening the URL in a browser:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/ You should see a response indicating the API is running (might be a 404 or a welcome message, depending on your root route configuration).\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-seedingdata/5.4.1-setup-script/","title":"Setup the Script","tags":[],"description":"","content":"Step 1: Navigate to the directory Navigate to the scripts directory where the seeding script is located:\ncd backend/scripts Step 2: Install dependencies The script uses the AWS SDK v3. Install the required packages:\nnpm install This will install all dependencies defined in the package.json file, including:\n@aws-sdk/client-cognito-identity-provider - For Cognito operations @aws-sdk/client-dynamodb - For DynamoDB operations dotenv - For environment variable management Other required dependencies Make sure you\u0026rsquo;re in the backend/scripts directory before running npm install.\nWait for the installation to complete successfully before proceeding to the next step.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get acquainted with First Cloud Journey team Learn AWS basics and setup environment Tasks carried out: Day Task Date Status Mon - Meet FCJ team - Read internship guidelines - Setup dev environment 11/04/2025 ‚úÖ Done Tue - Study AWS service categories (Compute, Storage, Networking, Database) - Review AWS global infrastructure 11/05/2025 ‚úÖ Done Wed - Create AWS Free Tier account - Setup MFA - Install AWS CLI v2 11/06/2025 ‚úÖ Done Thu - Learn EC2 basics (instance types, AMI, Security Groups) - Study SSH connection methods 11/07/2025 ‚úÖ Done Fri - Launch EC2 instance (t2.micro, Ubuntu) - Configure Security Group - SSH connection - Attach EBS volume 11/08/2025 ‚úÖ Done Achievements: ‚úÖ Successfully created AWS account with MFA ‚úÖ Installed and configured AWS CLI ‚úÖ Launched first EC2 instance ‚úÖ Connected via SSH successfully ‚úÖ Mounted EBS volume to /data Key Learnings: AWS service categories and use cases Security Group configuration importance Basic EC2 instance management AWS CLI commands for resource management Challenges: IAM permission issues ‚Üí Fixed by creating proper IAM user SSH timeout ‚Üí Added correct inbound rules to Security Group Next Week: VPC networking S3 storage IAM policies "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deep dive into VPC networking concepts Learn S3 storage service and best practices Understand IAM policies and roles Tasks carried out: Day Task Date Status Mon - Study VPC basics (subnets, route tables, internet gateway) - Learn about public vs private subnets - Security Groups vs NACLs 11/11/2025 ‚úÖ Done Tue - Create custom VPC with public/private subnets - Configure route tables - Setup NAT Gateway - Test EC2 in private subnet 11/12/2025 ‚úÖ Done Wed - Learn S3 fundamentals (buckets, objects, storage classes) - Study S3 security (bucket policies, ACLs) - S3 versioning and lifecycle policies 11/13/2025 ‚úÖ Done Thu - Create S3 buckets - Upload/download files via Console \u0026amp; CLI - Configure bucket policies - Enable versioning and encryption 11/14/2025 ‚úÖ Done Fri - Learn IAM basics (users, groups, roles, policies) - Study principle of least privilege - Create IAM policies for EC2 and S3 access 11/15/2025 ‚úÖ Done Achievements: ‚úÖ Created custom VPC with proper network segmentation ‚úÖ Deployed EC2 instances in public and private subnets ‚úÖ Configured NAT Gateway for private subnet internet access ‚úÖ Created and managed S3 buckets with proper security ‚úÖ Implemented IAM policies following least privilege principle Key Learnings: VPC architecture and subnet design best practices Difference between Security Groups (stateful) and NACLs (stateless) S3 storage classes and cost optimization strategies IAM policy structure and permission boundaries Challenges: NAT Gateway routing issues ‚Üí Fixed route table associations S3 bucket policy syntax errors ‚Üí Used AWS Policy Generator IAM permission denied errors ‚Üí Reviewed CloudTrail logs Next Week: RDS and database management CloudFormation basics Monitoring with CloudWatch "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand EC2 and S3 services in depth Learn to import virtual machines to AWS Export virtual machines from EC2 instance and AMI Mount file sharing on on-premise machine Tasks carried out: Day Task Date Status Mon - Learn EC2 basics (instance types, user data, metadata) - Study EC2 auto scaling (EFA/SR, Lightsail, MGN) 11/18/2025 ‚úÖ Done Tue - Create S3 bucket - Upload objects - Deploy web using S3 - Accelerate static web with CloudFront 11/19/2025 ‚úÖ Done Wed - Create Storage Gateway - Create File Shares - Mount File Shares to on-premise machine - Create backup plan - Set up notifications 11/20/2025 ‚úÖ Done Thu - Learn S3 advanced (access point, storage class, static website \u0026amp; CORS, Glacier, Snow Family, Storage Gateway, Backup) 11/21/2025 ‚úÖ Done Fri - Import virtual machine to AWS - Deploy instance from AMI - Export VM from EC2 instance and AMI through S3 bucket 11/22/2025 ‚úÖ Done Achievements: ‚úÖ Mastered EC2 instance types and scaling options ‚úÖ Created and configured S3 buckets for static website hosting ‚úÖ Deployed CloudFront distribution for content acceleration ‚úÖ Set up Storage Gateway for hybrid cloud storage ‚úÖ Successfully imported/exported virtual machines Key Learnings: EC2 user data and metadata usage S3 storage classes and cost optimization CloudFront CDN configuration Storage Gateway types (File, Volume, Tape) VM import/export process via AWS CLI Challenges: CORS configuration for S3 static website ‚Üí Fixed policy settings Storage Gateway connectivity issues ‚Üí Adjusted security groups VM import format errors ‚Üí Converted to supported OVA format Next Week: RDS database deployment Load balancing with ELB Auto Scaling groups "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand basic security on AWS Know how to manage resources by using tags Know to create IAM role and policy Tasks carried out: Day Task Date Status Mon - Create CloudFormation stack - Amazon Tag (DSD \u0026amp; ABAC) - Enable shadow copies, user storage quotas and continuous access share - Scale storage and throughput capacity 11/25/2025 ‚úÖ Done Tue - Learn security services on AWS (Shared Responsibility Model, IAM, Amazon Cognito, AWS Organizations, KMS) 11/26/2025 ‚úÖ Done Wed - Enable Security Hub - Create tags for instances - Create a role for Lambda function - Manage resources by using tags and resource groups 11/27/2025 ‚úÖ Done Thu - Create IAM policy and role - Create Restriction Policy and IAP limited user - Create key management service, AWS CloudTrail and Amazon Athena - Share encrypted data on S3 11/28/2025 ‚úÖ Done Fri - Create IAM Groups, IAM User - Configure role condition - Access to application through accesskey and IAM role on EC2 11/29/2025 ‚úÖ Done Achievements: ‚úÖ Understood AWS security fundamentals (Shared Responsibility Model, IAM, Cognito, Organizations, KMS) ‚úÖ Created and configured IAM roles and policies ‚úÖ Implemented resource tagging strategy for better management ‚úÖ Set up AWS Security Hub for security monitoring ‚úÖ Configured CloudTrail for audit logging Key Learnings: AWS Shared Responsibility Model principles IAM policy structure and best practices Resource tagging for cost allocation and organization Security Hub findings and compliance checks CloudTrail log analysis with Athena Challenges: IAM policy syntax errors ‚Üí Used AWS Policy Simulator for testing Tag enforcement ‚Üí Implemented Service Control Policies (SCPs) KMS key rotation ‚Üí Configured automatic key rotation Next Week: Load balancing and Auto Scaling CloudWatch monitoring and alarms Lambda serverless functions "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Review knowledge about VPC, EC2, S3, and Security Translate 3 blogs to Vietnamese Learn about minimax algorithm and AI game development Tasks carried out: Day Task Date Status Mon - Review modules 1-5 (VPC, EC2, S3, Security) - Consolidate AWS fundamentals knowledge 12/02/2025 ‚úÖ Done Tue - Translate 3 blogs: + Rox accelerates sales productivity with AI agents (Amazon Bedrock) + Laravel Nightwatch handles observability events (MSK \u0026amp; ClickHouse) + Export to Amazon S3 Tables using AWS Step Functions Distributed Map 12/03/2025 ‚úÖ Done Wed - Review React basics (Component, Props, React hook) - Learn minimax algorithm fundamentals 12/04/2025 ‚úÖ Done Thu - Practice: Create simple tic-tac-toe game - Implement minimax algorithm 12/05/2025 ‚úÖ Done Fri - Create AI tic-tac-toe with 2 difficulty modes (easy/hard) - Optimize game logic - Edit and finalize translated blogs 12/06/2025 ‚úÖ Done Achievements: ‚úÖ Reviewed and consolidated VPC, EC2, S3, and Security knowledge ‚úÖ Successfully translated 3 AWS technical blogs to Vietnamese ‚úÖ Understood minimax algorithm for game AI ‚úÖ Built functional tic-tac-toe game with AI opponent (2 difficulty levels) Key Learnings: AWS services integration patterns Technical translation best practices Minimax algorithm implementation React hooks and state management AI game development fundamentals Challenges: Technical terminology in blog translation ‚Üí Used AWS documentation for accuracy Minimax algorithm optimization ‚Üí Implemented alpha-beta pruning React state management ‚Üí Used useReducer for complex game state Next Week: AWS Lambda and serverless architecture DynamoDB database operations API Gateway integration "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand database concepts on AWS Know how to create DB and connect DB to EC2 Complete assignment of hackathon Tasks carried out: Day Task Date Status Mon - Learn about databases on AWS (RDS, Aurora, Redshift, ElastiCache) - Do lab 05 12/09/2025 ‚úÖ Done Tue - Practice: Create EC2 and RDS - Deploy application on EC2 and connect to RDS - Learn backup and restore 12/10/2025 ‚úÖ Done Wed - Learn about distributed systems (network, TCP, HTTP, WebSocket) - Review system architecture patterns 12/11/2025 ‚úÖ Done Thu - Practice: Create multiplayer odd/even tic-tac-toe game - Implement real-time communication 12/12/2025 ‚úÖ Done Fri - Practice: Continue fix bugs and complete game - Deploy and test final version 12/13/2025 ‚úÖ Done Achievements: ‚úÖ Understood fundamental database concepts (RDBMS, NoSQL, OLTP, OLAP) ‚úÖ Learned AWS database services (RDS, Aurora, Redshift, ElastiCache) ‚úÖ Successfully created and configured Amazon RDS instance ‚úÖ Connected EC2 to RDS and deployed application ‚úÖ Created multiplayer odd/even tic-tac-toe game with real-time features Key Learnings: AWS RDS setup and configuration Database security groups and connectivity RDS backup and restore mechanisms Distributed systems fundamentals (TCP, HTTP, WebSocket) Real-time multiplayer game architecture Challenges: RDS security group configuration ‚Üí Fixed inbound rules for EC2 access WebSocket connection stability ‚Üí Implemented reconnection logic Game state synchronization ‚Üí Used proper state management patterns Next Week: Container services (ECS, EKS) Docker basics Microservices architecture "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete labs of module 7 Complete assignment of Hackathon Tasks carried out: Day Task Date Status Mon - Review database concepts knowledge - Do labs 35 and 40 12/16/2025 ‚úÖ Done Tue - Learn about Agile (Scrum, Kanban) - Practice: Write product vision and pitch 12/17/2025 ‚úÖ Done Wed - Do lab 60 and 70 - Practice: Use AWS Glue to scan data in S3 - Connect Amazon Athena to AWS Glue - Connect Amazon QuickSight to Amazon Athena 12/18/2025 ‚úÖ Done Thu - Practice: Use SDK to create table, CRUD data in DynamoDB - Build a database with data stored in S3 12/19/2025 ‚úÖ Done Fri - Do labs 72 and 75 - Practice: Use AWS Glue to visually profile, clean and transform raw data - Build dashboard via QuickSight 12/20/2025 ‚úÖ Done Achievements: ‚úÖ Successfully created product vision and pitch ‚úÖ Understood serverless data lake and analytics architecture (AWS Glue, Amazon Athena, Amazon QuickSight) ‚úÖ Designed and published interactive dashboards using Amazon QuickSight ‚úÖ Used Amazon Athena to analyze S3 data with SQL Key Learnings: Agile methodologies (Scrum, Kanban) AWS Glue for ETL operations Athena for serverless SQL queries QuickSight for business intelligence dashboards DynamoDB SDK operations Challenges: Glue crawler configuration ‚Üí Fixed schema detection settings Athena query optimization ‚Üí Used partitioning and compression QuickSight data source connection ‚Üí Configured proper IAM permissions Next Week: Lambda functions API Gateway Serverless application architecture "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review knowledge from modules 1 to 7 Learn about Naver Clova Studio Tasks carried out: Day Task Date Status Mon - Review AWS knowledge (Cost Optimization, EC2 basics) 12/23/2025 ‚úÖ Done Tue - Learn about Naver Clova Studio - Explore ideas for Hackathon - Review EC2 concepts 12/24/2025 ‚úÖ Done Wed - Review S3 and Shared Responsibility Model 12/25/2025 ‚úÖ Done Thu - Review data concepts - Do practice exams from Notebooklm 12/26/2025 ‚úÖ Done Fri - Do exams on Notebooklm - Review all knowledge from modules 1 to 7 12/27/2025 ‚úÖ Done Achievements: ‚úÖ Understood Naver Clova Studio capabilities and features ‚úÖ Successfully chose Hackathon idea (app for supporting deaf persons to communicate with others) ‚úÖ Reviewed all knowledge from modules 1 to 7 ‚úÖ Completed practice exams and assessments Key Learnings: Naver Clova Studio AI services Cost optimization strategies on AWS Comprehensive AWS fundamentals review Exam preparation and assessment techniques Challenges: Choosing Hackathon project scope ‚Üí Focused on accessibility features Time management for review ‚Üí Created structured review schedule Exam preparation ‚Üí Used Notebooklm for practice tests Next Week: Hackathon implementation Advanced AWS services Project development "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Build demo web Build connect S3 and CloudFront for workshop Discover AI model for Hackathon project Tasks carried out: Day Task Date Status Mon - Create basic app, deploy fe on S3 and CloudFront 01/03/2025 ‚úÖ Done Tue - Create API gateway and lambda function for login logout - Fix bugs about CORS 01/04/2025 ‚úÖ Done Wed - Discover about model AI - Learn how to run this model 01/05/2025 ‚úÖ Done Thu - Create a simple web with camera for project - Discover about new model VSL 01/06/2025 ‚úÖ Done Fri - Implement model AI VSL into project 01/07/2025 ‚úÖ Done Achievements: ‚úÖ Successfully built simple web for workshop and hackathon ‚úÖ Successfully connect S3 and CloudFront to deploy fe ‚úÖ Successfully implement model AI VSL into web Key Learnings: S3 static website hosting configuration CloudFront distribution setup and caching API Gateway and Lambda integration CORS handling in serverless architecture AI model integration (VSL - Vietnamese Sign Language) Challenges: CORS configuration ‚Üí Fixed API Gateway and Lambda response headers CloudFront caching issues ‚Üí Set proper cache invalidation AI model integration ‚Üí Optimized model loading and inference Next Week: Complete Hackathon project Optimize application performance Final testing and deployment "},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nDuring the internship I built a foundation in AWS Cloud and gradually worked toward a capstone project:\nan AWS Serverless AI Platform for Smart Accommodation Search.\nTypically, my weekly contents are organized as follows:\nWeek 1: Getting familiar with AWS and core services\nIntroduction to the AWS console, IAM, regions and AZs, basic EC2 and S3 usage.\nWeek 2: Networking and security fundamentals\nVPC, subnets, security groups, NACLs, key security best practices.\nWeek 3: Compute and storage deep dive\nEC2 types, EBS, S3 advanced features (lifecycle, versioning), cost-awareness.\nWeek 4: Relational databases on AWS\nAmazon RDS basics, backup/restore, high availability options.\nWeek 5: Data processing and analytics\nIntroduction to AWS Glue, Athena, and data lake concepts.\nWeek 6: Visualization with Amazon QuickSight\nBuilding dashboards and reports from curated datasets.\nWeek 7: Serverless foundations\nAWS Lambda, API Gateway, event‚Äëdriven patterns, basic monitoring with CloudWatch.\nWeek 8: Serverless patterns for web backends\nDesigning simple APIs and integrating with databases in a serverless way.\nWeek 9: Designing the AWS Serverless AI Platform\nDefining requirements, architecture diagram, and data model for the capstone project.\nWeek 10: Implementing backend and data layer\nBuilding Lambda functions, APIs, and data storage for accommodation information.\nWeek 11: Integrating AI recommendations\nConnecting to an AI/ML model to rank and recommend accommodations based on user preferences.\nWeek 12: Hardening, monitoring, and cost optimization\nImproving security, adding logging/metrics, and reviewing cost.\nWeek 13: Final review and documentation\nWriting documentation, polishing the demo, and reflecting on lessons learned from the whole internship.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-seedingdata/5.4.2-configure-environment/","title":"Configure Environment","tags":[],"description":"","content":"The script (seed-admin.js) looks for specific environment variables to connect to your AWS resources. You need to create a .env file with values from the CDK Deployment Output.\nCreate the .env file Create a file named .env in the backend/scripts folder with the following content:\n# .env file content REGION=us-east-1 USER_POOL_ID=\u0026lt;Paste_Value_From_CDK_Output\u0026gt; USER_PROFILES_TABLE_NAME=UserProfiles Environment Variables Explained Variable Description Where to find REGION AWS region where resources are deployed us-east-1 (or ap-southeast-1 if you changed it) USER_POOL_ID Cognito User Pool identifier Copy the value of BackendStack.UserPoolId from the terminal after cdk deploy USER_PROFILES_TABLE_NAME DynamoDB table name for user profiles In your CDK code, we set the table name to UserProfiles. If you changed it, check the DynamoDB Console for the exact name Example Configuration # Example .env file REGION=us-east-1 USER_POOL_ID=us-east-1_AbCdEfGh USER_PROFILES_TABLE_NAME=UserProfiles Make sure to replace \u0026lt;Paste_Value_From_CDK_Output\u0026gt; with the actual value from your CDK deployment outputs!\nYou can find the CDK outputs by scrolling up in your terminal or by running cdk deploy again (it won\u0026rsquo;t redeploy if nothing changed).\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-deploybackend/5.3.2-install-dependencies/","title":"Install Dependencies","tags":[],"description":"","content":"The system consists of two parts of source code requiring dependencies.\nStep 1: Install for Backend Navigate to the backend Lambda directory and install the required Node.js packages:\ncd backend/src/lambda npm install This will install all dependencies defined in the package.json file for the Lambda function.\nStep 2: Install for CDK Go back to the CDK root directory and install CDK dependencies:\n# Go back to CDK root directory cd ../../../cdk npm install This installs the AWS CDK libraries and constructs needed to synthesize and deploy the infrastructure.\nMake sure you have Node.js and npm installed on your machine before running these commands.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Install Dependencies","tags":[],"description":"","content":"The system consists of two parts of source code requiring dependencies.\nStep 1: Install for Backend Navigate to the backend Lambda directory and install the required Node.js packages:\ncd backend/src/lambda npm install This will install all dependencies defined in the package.json file for the Lambda function.\nStep 2: Install for CDK Go back to the CDK root directory and install CDK dependencies:\n# Go back to CDK root directory cd ../../../cdk npm install This installs the AWS CDK libraries and constructs needed to synthesize and deploy the infrastructure.\nMake sure you have Node.js and npm installed on your machine before running these commands.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions To deploy the Findnest Backend stack securely, adhering to the principle of least privilege, add the following JSON policy to your IAM User.\nThis policy focuses strictly on the services defined in your CDK code (DynamoDB, Cognito, Location, etc.)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;FindnestDeploymentPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34;, \u0026#34;geo:*\u0026#34;, \u0026#34;sns:*\u0026#34;, \u0026#34;bedrock:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources (CDK Bootstrap) In this lab, we will use N. Virginia region (us-east-1). To prepare the workshop environment, we need to provision the CDK Bootstrap resources. This process creates an S3 Bucket to store your Lambda code and CloudFormation templates.\n1. Install Dependencies\nEnsure you have the core tools installed on your local machine:\nnpm install -g aws-cdk 2. Bootstrap the Environment\nRun the following command in your terminal to deploy the bootstrap stack. Accept all of the defaults.\ncdk bootstrap aws://\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/us-east-1 (Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your 12-digit AWS Account ID)\nWait for the CloudFormation stack CDKToolkit to reach CREATE_COMPLETE status. 1 S3 Bucket has been created to store assets. The environment is now ready for serverless deployment.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"FindNest AWS Serverless AI Platform for Smart Accommodation Search 1. Executive Summary The FindNest platform leverages AI understanding and AWS Serverless architecture to transform accommodation search into a contextual, intelligent experience. By combining Amazon Bedrock for natural language processing and Amazon Location Service for spatial analysis, it enables users to find rooms using natural queries like \u0026ldquo;affordable room near Thu Duc with gym and safe area.\u0026rdquo;\nThe system (Frontend: React hosted on Amplify, Backend: AWS Lambda + API Gateway) stores data in DynamoDB, handles authentication via Cognito, and enriches listings with contextual insights such as food density, nearby amenities, and safety index. Notifications and OTP authentication are handled via Amazon SNS. The entire platform operates under AWS Free Tier with an estimated cost of ~$0.5/month.\n2. Problem Statement What\u0026rsquo;s the Problem? Current platforms only support simple filters such as price or area and lack the ability to understand nuanced user intent. Users must manually review hundreds of listings without AI assistance or location intelligence. There‚Äôs no mechanism to understand complex preferences like ‚Äúsafe neighborhood with good food options‚Äù or ‚Äúeasy commute to District 1.‚Äù\nThe Solution An AI-enhanced, serverless platform that interprets user intent through natural language, automatically enriches listings with contextual data (restaurants, safety, routes), and recommends relevant results using Amazon Bedrock and Amazon Location Service. The backend filters listings stored in DynamoDB and ranks results using AI scoring.\nBenefits and Return on Investment AI Semantic Search: Bedrock interprets user intent beyond filters. Contextual Recommendations: Listings enriched by Location Service provide real-world relevance. Serverless Scalability: Fully managed services scale automatically with zero maintenance. Cost Efficiency: All components run within AWS Free Tier for MVP phase (~$0.5/month). 3. Solution Architecture The platform utilizes a modular AWS Serverless design with AI enrichment and dynamic contextual data.\nComponent Service / Technology Frontend Hosting AWS Amplify (React SPA) API Backend AWS Lambda + API Gateway Database DynamoDB File Storage S3 User Management Cognito Notifications Amazon SNS Map \u0026amp; Location Amazon Location Service Recommendation Engine Amazon Bedrock + Lambda Logic AWS Services Used AWS Lambda: Executes backend logic including AI processing and search queries. Amazon API Gateway: Provides RESTful endpoints for client requests. Amazon DynamoDB: Stores user profiles, listings, and enriched context data. Amazon S3: Stores room images and frontend static files. AWS Amplify: Hosts and manages frontend deployment. Amazon Cognito: Manages authentication and authorization flows. Amazon SNS: Sends OTP codes and user notifications. Amazon Location Service: Fetches surrounding POIs, routes, and safety context. Amazon Bedrock: Interprets natural language search and performs semantic ranking. Component Design Frontend Application: React SPA hosted on Amplify for responsive, dynamic user experience. API Layer: Express app deployed on Lambda via API Gateway handling AI search, listings, and enrichment. Database: DynamoDB tables for listings, users, and search history. Storage: S3 bucket stores images; public read via signed URLs. Recommendation Engine: Bedrock interprets user queries and ranks listings. Map Integration: Amazon Location Service provides contextual location and POI visualization. Notification System: SNS delivers OTPs and alerts for new listings. User Management: Cognito handles registration, login, and secure tokens. 4. Technical Implementation Recommendation Engine Approach\nMVP Phase: Bedrock interprets user query ‚Üí DynamoDB filters + Location Service enrichment. Expansion Phase: Continuous enrichment jobs to compute contextual indexes (food_density, safety_score, comfort_index). Advanced Phase: Adaptive learning ‚Äî store user feedback to refine Bedrock prompt responses. Technical Requirements\nFrontend: React + Amplify UI with AI-integrated search bar and location maps. Backend: Node.js Lambda app using AWS SDK for Bedrock, DynamoDB, and Location. Database: DynamoDB tables for users, listings, and search history. Storage: S3 buckets for file storage. Authentication: Cognito + SNS OTP login flow. 5. Timeline \u0026amp; Milestones Project Timeline\nWeek 1-2: Design AWS architecture, configure Amplify, and deploy base API. Week 3-4: Implement Bedrock semantic search and Location enrichment logic. Week 5: Integrate frontend and refine AI-driven search flow. Week 6: Finalize testing and deployment. Post-Launch: Collect user data for AI improvement and feedback loops. 6. Budget Estimation Infrastructure Costs Component Service Estimated Cost Lambda + API Gateway Backend $0.22/month DynamoDB Database $0.10/month S3 Storage $0.20/month Cognito + SNS Authentication + OTP $0.13/month Bedrock AI Processing $7.5/month Location Service Map \u0026amp; Geospatial Data $3.30 Total ~$24.32/month Note: All services operate under Free Tier usage limits during MVP phase with minimal operational cost.\n7. Risk Assessment Risk Matrix Bedrock Misinterpretation: Medium impact, medium probability. Lambda Cost Spike (Scaling): Low impact, medium probability. Incomplete Contextual Data: Medium impact, low probability. Mitigation Strategies Prompt Engineering: Optimize Bedrock input and fallback to simple filters. Caching: Cache Location and AI results for frequent queries. Contingency Plans Bedrock Limitations: Fallback to DynamoDB-only filter logic. Timeout Issues: Split enrichment jobs into smaller Lambda batches. High API Load: Scale API Gateway with usage throttling. 8. Expected Outcomes Technical Improvements AI-powered natural language search via Bedrock. Automatic contextual enrichment using Location Service. Scalable, low-cost infrastructure powered by AWS Serverless stack. Long-term Value Continuous Learning: Improve Bedrock prompts with user feedback. Smart Context Awareness: Build dynamic profiles for regions and user habits. Scalable Foundation: Ready for integration with Amazon Personalize or Bedrock fine-tuning. Cost Efficiency: Fully serverless with minimal maintenance and no fixed servers. Download the Proposal\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/5.5.2-test-admin-login/","title":"Test Admin Login","tags":[],"description":"","content":"Test Case 1: Admin Login This is the most critical test. It verifies:\n‚úÖ API Gateway correctly routes traffic to Lambda ‚úÖ Lambda can connect to Cognito ‚úÖ Cognito validates the credentials seeded in Section 5.4 Request Details Method: POST\nURL: \u0026lt;ApiUrl\u0026gt;/auth/login\nBody (JSON):\n{ \u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34; } Replace the email and password with the credentials you created in Section 5.4 (Seeding Data).\nExecute with cURL curl -X POST \u0026lt;YOUR_API_URL\u0026gt;/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34;}\u0026#39; Example:\ncurl -X POST https://xyz123.execute-api.us-east-1.amazonaws.com/prod/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34;}\u0026#39; Expected Response (200 OK) You should receive a JSON object containing the authentication tokens (ID Token, Access Token):\n{ \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;accessToken\u0026#34;: \u0026#34;eyJraWQiOiJ...\u0026#34;, \u0026#34;refreshToken\u0026#34;: \u0026#34;eyJjdHkiOiJ...\u0026#34;, \u0026#34;idToken\u0026#34;: \u0026#34;eyJraWQiOiJ...\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admins\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;Login successful\u0026#34; } Important Action Copy the accessToken from the response. We will need it for the next test case.\nIf you receive the tokens, congratulations! Your authentication system is working correctly.\nTroubleshooting If you encounter errors:\n400 Bad Request: Check that the JSON body is formatted correctly 401 Unauthorized: Verify the email and password match what you created in Section 5.4 500 Internal Server Error: Check Lambda logs in CloudWatch Logs Network Error: Verify the API URL is correct and accessible Using Postman If you prefer using Postman:\nCreate a new POST request Set URL to \u0026lt;YOUR_API_URL\u0026gt;/auth/login Go to Body tab ‚Üí Select raw ‚Üí Choose JSON Paste the JSON body Click Send Copy the accessToken from the response "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.6-cleanup/5.6.2-verify-deletion/","title":"Verify Resource Deletion","tags":[],"description":"","content":"Although we configured RemovalPolicy.DESTROY and autoDeleteObjects: true in our CDK code, it is a Best Practice to double-check the AWS Console to ensure everything is gone.\nA. Check Amazon S3 Go to the S3 Console Search for findnest in the bucket list Verify the bucket findnest-images-... is deleted If the bucket still exists:\nSelect the bucket Click Empty button Confirm by typing permanently delete After emptying, click Delete button Confirm by typing the bucket name The autoDeleteObjects: true setting in our code usually handles this automatically by deploying a custom Lambda function to clear the bucket before deletion.\nB. Check DynamoDB Tables Go to the DynamoDB Console Click Tables in the left sidebar Verify that all tables are deleted: BoardingHouseListings UserProfiles OTPVerifications UserFavorites SupportRequests SearchHistory UserPreferences If any tables remain, select them and click Delete.\nC. Check CloudWatch Logs Go to the CloudWatch Console Navigate to Logs ‚Üí Log groups Search for /aws/lambda/FindNestApi If the log group persists (sometimes logs generated during deletion are kept):\nSelect the log group Choose Actions ‚Üí Delete log group(s) Confirm deletion Log groups might accumulate small charges over time. It\u0026rsquo;s good practice to delete them if no longer needed.\nD. Check Lambda Functions Go to the Lambda Console Verify that FindNestApi function is deleted If it still exists, select it and click Actions ‚Üí Delete.\nE. Check API Gateway Go to the API Gateway Console Verify that FindNestAPI is deleted If it still exists, select it and click Actions ‚Üí Delete API.\nF. Check Cognito Go to the Cognito Console Check User Pools - Verify FindNestUsers is deleted Check Identity Pools - Verify FindNestMapAccess is deleted G. Check Location Service Go to the Amazon Location Service Console Verify the following are deleted: Map: FindNestMap Place Index: FindNestPlaces Route Calculator: FindNestRoutes H. Check CloudFormation Go to the CloudFormation Console Verify that BackendStack shows status DELETE_COMPLETE or is no longer listed If you don\u0026rsquo;t see the stack or it shows DELETE_COMPLETE, your cleanup is successful!\nVerification Checklist Confirm all resources are deleted:\n‚úÖ S3 Bucket (findnest-images-*) ‚úÖ DynamoDB Tables (all 7 tables) ‚úÖ Lambda Function (FindNestApi) ‚úÖ API Gateway (FindNestAPI) ‚úÖ Cognito User Pool (FindNestUsers) ‚úÖ Cognito Identity Pool (FindNestMapAccess) ‚úÖ Location Service resources (Map, Place Index, Route Calculator) ‚úÖ CloudWatch Log Groups (/aws/lambda/FindNestApi) ‚úÖ CloudFormation Stack (BackendStack) ‚úÖ IAM Roles (auto-deleted with stack) If any resources remain after 10 minutes, manually delete them to avoid unexpected charges.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Change project into gitlab Configure CI/CD for project Discover how to translate continuously words with model AI Tasks carried out: Day Task Date Status Mon - Find way that how to model can translate long video with a lot of gestures 01/10/2025 ‚úÖ Done Tue - Change project from Github to Gitlab - Learn how to use Gitlab - Configure CI/CD 01/11/2025 ‚úÖ Done Wed - Implement sliding window in be logic to translate continuously words 01/12/2025 ‚úÖ Done Thu - Fix bugs and test web 01/13/2025 ‚úÖ Done Fri - Implement paused-based segmentation in be 01/14/2025 ‚úÖ Done Achievements: ‚úÖ Successfully configure CI/CD for gitlab to S3 and Cloudfront ‚úÖ Implement two ways about translate continuous words into web ‚úÖ Test accuracy of two ways Key Learnings: Gitlab CI/CD pipeline configuration Automated deployment to S3 and CloudFront AI model optimization for continuous translation Sliding window algorithm implementation Paused-based segmentation techniques Challenges: Gitlab CI/CD pipeline setup ‚Üí Configured proper stages and deployment scripts Continuous translation accuracy ‚Üí Implemented sliding window approach Model performance ‚Üí Optimized with paused-based segmentation Next Week: Final Hackathon presentation Project documentation Performance optimization "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-deploybackend/","title":"Deploy Backend","tags":[],"description":"","content":"Introduction to Infrastructure as Code (IaC) Instead of manually clicking through the AWS Console (\u0026ldquo;ClickOps\u0026rdquo;), we use AWS CDK to define our entire infrastructure. In the file cdk/lib/backend-stack.ts, we have designed a complete Serverless system.\nWhen you run the cdk deploy command, CDK synthesizes this code into a CloudFormation Template, and AWS automatically provisions the corresponding resources.\nContent Architecture Deep Dive Install Dependencies Deploy Stack Results \u0026amp; Outputs "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-deploybackend/5.3.3-deploy-stack/","title":"Deploy Stack","tags":[],"description":"","content":"Now, let\u0026rsquo;s turn this design into reality.\nStep 1: Synthesize Run the synthesize command to generate the CloudFormation template:\ncdk synth This command generates the CloudFormation template in the cdk.out directory. If no red errors appear, you may proceed.\nThe synthesize process converts your CDK code (TypeScript) into a CloudFormation template (JSON/YAML).\nStep 2: Deploy Deploy the stack to AWS:\ncdk deploy --require-approval never The --require-approval never flag bypasses the confirmation prompt for IAM changes.\nThe deployment process will take approximately 5-10 minutes to provision all resources.\nDuring deployment, you\u0026rsquo;ll see:\nCloudFormation stack creation progress Individual resources being created (DynamoDB tables, Lambda functions, API Gateway, etc.) Status updates for each resource Wait for the deployment to complete successfully before proceeding to the next step.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-s3-vpc/5.3.3-deploy-stack/","title":"Deploy Stack","tags":[],"description":"","content":"Now, let\u0026rsquo;s turn this design into reality.\nStep 1: Synthesize Run the synthesize command to generate the CloudFormation template:\ncdk synth This command generates the CloudFormation template in the cdk.out directory. If no red errors appear, you may proceed.\nThe synthesize process converts your CDK code (TypeScript) into a CloudFormation template (JSON/YAML).\nStep 2: Deploy Deploy the stack to AWS:\ncdk deploy --require-approval never The --require-approval never flag bypasses the confirmation prompt for IAM changes.\nThe deployment process will take approximately 5-10 minutes to provision all resources.\nDuring deployment, you\u0026rsquo;ll see:\nCloudFormation stack creation progress Individual resources being created (DynamoDB tables, Lambda functions, API Gateway, etc.) Status updates for each resource Wait for the deployment to complete successfully before proceeding to the next step.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-seedingdata/5.4.3-run-script/","title":"Run the Script","tags":[],"description":"","content":"Now, execute the script. It is interactive, so it will ask you for details.\nExecute the script node seed-admin.js Interactive Walkthrough The script will prompt you for the following information:\nUsername: Enter a username (e.g., superadmin) Password: Enter a strong password (min 8 chars, uppercase, lowercase, numbers, special chars) Email: Enter a valid email address Full Name: Enter a display name Confirm: Type yes to proceed Expected Output If everything is configured correctly, you should see output similar to this:\n‚úÖ Environment variables loaded: Region: us-east-1 User Pool ID: us-east-1_xxxxxx User Profiles Table: UserProfiles üìù Creating admin user in Cognito... ‚úÖ User created with ID: xxxx-xxxx-xxxx üîë Setting permanent password... ‚úÖ Password set üëë Adding user to Admins group... ‚úÖ Added to Admins group üìù Creating admin profile in DynamoDB... ‚úÖ Profile created ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚úÖ ADMIN USER CREATED SUCCESSFULLY! ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Save the credentials! You\u0026rsquo;ll need the username and password to log in to the system.\nIf you encounter errors:\nCheck that your .env file has the correct values Verify your AWS credentials are configured (aws configure) Ensure you have the necessary IAM permissions Make sure the CDK stack was deployed successfully What the script does The script performs the following operations:\n‚úÖ Creates a user in Cognito User Pool with the provided username and password ‚úÖ Sets the password as permanent (no need to change on first login) ‚úÖ Adds the user to the \u0026ldquo;Admins\u0026rdquo; group for elevated permissions ‚úÖ Creates a profile record in DynamoDB with user information "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/5.5.3-test-create-listing/","title":"Test Create Listing","tags":[],"description":"","content":"Test Case 2: Create a Listing (Protected Route) Now, let\u0026rsquo;s test if the permissions are working. Only authenticated users (Landlords/Admins) can create a boarding house listing.\nThis tests Lambda\u0026rsquo;s connection to DynamoDB.\nRequest Details Method: POST\nURL: \u0026lt;ApiUrl\u0026gt;/landlord/listings\nHeaders:\nAuthorization: Bearer \u0026lt;YOUR_ACCESS_TOKEN\u0026gt; Content-Type: application/json Body (JSON):\n{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;] } Replace \u0026lt;YOUR_ACCESS_TOKEN\u0026gt; with the token you copied from the previous test (Admin Login).\nExecute with cURL curl -X POST \u0026lt;YOUR_API_URL\u0026gt;/landlord/listings \\ -H \u0026#34;Authorization: Bearer \u0026lt;PASTE_TOKEN_HERE\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;] }\u0026#39; Example:\ncurl -X POST https://xyz123.execute-api.us-east-1.amazonaws.com/prod/landlord/listings \\ -H \u0026#34;Authorization: Bearer eyJraWQiOiJxxx...\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;] }\u0026#39; Expected Response (201 Created) { \u0026#34;message\u0026#34;: \u0026#34;Listing created successfully\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;listingId\u0026#34;: \u0026#34;listing_12345...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;], \u0026#34;createdAt\u0026#34;: \u0026#34;2023-10-27T10:30:00Z\u0026#34; } } If you receive a 201 Created response with the listing data, your protected route and DynamoDB integration are working correctly!\nWhat This Test Validates ‚úÖ JWT Token Verification: Lambda validates the access token ‚úÖ Authorization: Only authenticated users can create listings ‚úÖ DynamoDB Write: Lambda successfully writes data to DynamoDB ‚úÖ Data Persistence: The listing is stored in the database Troubleshooting Common errors:\n401 Unauthorized: Token is invalid or expired. Login again to get a fresh token 403 Forbidden: User doesn\u0026rsquo;t have the required permissions 400 Bad Request: Check the JSON body format and required fields 500 Internal Server Error: Check Lambda logs for DynamoDB permission issues Using Postman If using Postman:\nCreate a new POST request Set URL to \u0026lt;YOUR_API_URL\u0026gt;/landlord/listings Go to Headers tab: Add Authorization: Bearer \u0026lt;YOUR_TOKEN\u0026gt; Add Content-Type: application/json Go to Body tab ‚Üí Select raw ‚Üí Choose JSON Paste the JSON body Click Send "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://hainam1007.github.io/aws-ojt-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section lists and briefly introduces the AWS blogs you have summarized and translated.\nBlog 1 ‚Äì Build agentic systems with CrewAI and Amazon Bedrock This blog explains how to build multi‚Äëagent (‚Äúagentic‚Äù) applications using the open‚Äësource CrewAI framework together with Amazon Bedrock. It introduces key concepts such as agents, tools, tasks, and flows, shows how to orchestrate multiple specialized agents for complex workflows, and presents a reference solution for cloud‚Äësecurity posture management on AWS using Bedrock models, Agents, and Knowledge Bases.\nBlog 2 ‚Äì Accelerate your AWS migration with AWS Training and Certification This blog focuses on how AWS Training and Certification supports each phase of a cloud‚Äëmigration journey: Assess, Mobilize, and Migrate \u0026amp; Modernize. It highlights common challenges (leadership alignment, migration strategy, lack of cloud skills) and maps them to specific learning resources and programs‚Äîsuch as AWS Technical Essentials, Migration Essentials, Knowledge Badges, and Skill Builder‚Äîto help organizations build the right capabilities and sustain long‚Äëterm success on AWS.\nBlog 3 ‚Äì Reserve your seat: Microsoft workloads on AWS sessions at re:Invent 2024 This blog is a guide to re:Invent 2024 sessions dedicated to Microsoft workloads on AWS, including Windows Server, SQL Server, Active Directory, and .NET. It groups sessions by format (breakout sessions, builders‚Äô sessions, chalk talks, code talks, and workshops), summarizes what attendees can learn‚Äîfrom cost optimization and licensing to modernization and generative‚ÄëAI use cases‚Äîand encourages readers to reserve seats early and visit the Microsoft workloads area in the expo.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Build MVP Hackathon project Create API gateway and lambda function for workshop Tasks carried out: Day Task Date Status Mon - Participate in AWS Cloud Mastery Series #2 11/17/2025 ‚úÖ Done Tue - Create forget password function for web - Fix bugs lambda and CORS 11/18/2025 ‚úÖ Done Wed - Implement split 3s for video on fe and testing 11/19/2025 ‚úÖ Done Thu - Implement Web Speech API in project and continue testing - Learn Coursera 11/20/2025 ‚úÖ Done Fri - Enhance UI of web for function record video and split 3s - Learn Coursera 11/21/2025 ‚úÖ Done Achievements: ‚úÖ Successfully create forget password for workshop ‚úÖ Successfully create TalkSign project for hackathon ‚úÖ Participated in AWS Cloud Mastery Series event Key Learnings: Lambda function development for authentication Web Speech API integration Video processing and splitting techniques UI/UX enhancement for accessibility features Challenges: Lambda CORS configuration ‚Üí Fixed response headers Video splitting performance ‚Üí Optimized frontend processing Web Speech API compatibility ‚Üí Tested across browsers Next Week: Final Hackathon presentation Workshop completion Project documentation "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.6-cleanup/5.6.3-conclusion/","title":"Workshop Conclusion","tags":[],"description":"","content":"Congratulations You have successfully completed the Findnest Serverless Backend Workshop!\nWhat Have You Achieved? 1. Modern Architecture\nYou deployed a fully Serverless architecture using:\n‚úÖ AWS Lambda - Serverless compute for backend logic ‚úÖ API Gateway - RESTful API endpoint management ‚úÖ DynamoDB - NoSQL database with on-demand scaling ‚úÖ Amazon Cognito - User authentication and authorization ‚úÖ Amazon Location Service - Maps and geocoding capabilities ‚úÖ Amazon Bedrock - AI/ML integration with Claude 3 ‚úÖ Amazon SNS - SMS notifications for OTP ‚úÖ Amazon S3 - Object storage for images 2. Infrastructure as Code\nYou used AWS CDK (TypeScript) to:\nDefine and provision cloud resources professionally Avoid manual \u0026ldquo;ClickOps\u0026rdquo; errors Enable version control for infrastructure Deploy and destroy entire stacks with single commands Implement proper resource dependencies 3. Advanced Integration\nYou integrated specialized services:\nAuthentication: Multi-factor auth with phone verification Database: 7 DynamoDB tables with proper indexes and TTL Storage: Secure S3 buckets with automatic cleanup Geolocation: Maps, place search, and route calculation AI/ML: Claude 3 for intelligent recommendations Notifications: SMS via SNS for verification codes 4. Operational Best Practices\nYou applied:\n‚úÖ Least Privilege Permissions - Granular IAM policies ‚úÖ Automated Cleanup - RemovalPolicy and autoDeleteObjects ‚úÖ Environment Variables - Configuration management ‚úÖ Logging - CloudWatch integration ‚úÖ CORS Configuration - Secure cross-origin requests ‚úÖ API Throttling - Rate limiting protection Skills Acquired By completing this workshop, you now have hands-on experience with:\nServerless Architecture Design\nEvent-driven patterns Stateless compute Managed services integration AWS CDK Development\nTypeScript constructs Resource provisioning Stack management API Development\nRESTful endpoints Authentication flows Protected and public routes Database Design\nNoSQL data modeling DynamoDB best practices TTL and indexes Security Implementation\nIAM policies Cognito user pools JWT token validation DevOps Practices\nInfrastructure as Code Automated deployments Resource cleanup Next Steps Continue your learning journey:\nEnhance the Backend:\nAdd more features (reviews, payments, real-time chat) Implement caching with Amazon ElastiCache Add API versioning Set up CloudWatch alarms and monitoring Implement CI/CD pipeline with AWS CodePipeline Build the Frontend:\nCreate a React Native mobile app Integrate with the API you just built Implement map visualization Add real-time notifications Production Readiness:\nSet up multiple environments (dev, staging, prod) Implement secrets management with AWS Secrets Manager Add comprehensive error handling Set up automated testing Configure custom domain with Route 53 Learn More AWS Services:\nAWS AppSync for GraphQL APIs Amazon EventBridge for event-driven architecture AWS Step Functions for workflow orchestration Amazon SQS for message queuing Resources AWS CDK Documentation AWS Lambda Best Practices DynamoDB Design Patterns API Gateway Documentation Thank You You are now equipped with the knowledge to build scalable, secure, and cost-effective backends on AWS.\nHappy Building! üöÄ\nKeep experimenting, keep learning, and keep building amazing things with AWS!\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://hainam1007.github.io/aws-ojt-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event\u0026rsquo;s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship at First Cloud Journey, I participated in two comprehensive AWS training events that significantly enhanced my cloud computing skills and practical knowledge.\nEvent 1: AI/ML/GenAI on AWS Workshop Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025 | 8:30 AM ‚Äì 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription:\nHands-on workshop covering AWS AI services (Rekognition, Polly, Transcribe, Comprehend) and Generative AI with Amazon Bedrock. Learned prompt engineering, RAG architecture, and built a chatbot using foundation models.\nKey Outcomes:\nGained practical experience with AWS AI/ML services Learned prompt engineering techniques for better AI responses Built a working chatbot using Amazon Bedrock Understood RAG architecture for knowledge-base applications Event 2: DevOps on AWS Training Event Name: DevOps on AWS Training Session\nDate \u0026amp; Time: Monday, November 17, 2025 | 8:30 AM ‚Äì 5:00 PM\nLocation: AWS Office\nRole: Attendee\nDescription:\nFull-day intensive training on DevOps practices with AWS. Covered CI/CD pipelines (CodeCommit, CodeBuild, CodeDeploy, CodePipeline), Infrastructure as Code (CloudFormation, CDK), container orchestration (ECS, EKS), and monitoring (CloudWatch, X-Ray).\nKey Outcomes:\nBuilt complete CI/CD pipelines from source to production Implemented Infrastructure as Code using CDK and CloudFormation Deployed containerized applications to ECS Fargate Configured full-stack monitoring and observability Learned deployment strategies (Blue/Green, Canary) Summary Both events provided invaluable hands-on experience with AWS services. The AI/ML workshop introduced cutting-edge generative AI technologies, while the DevOps training equipped me with modern deployment practices. Together, they formed a comprehensive foundation for building and operating cloud applications on AWS.\nOverall Impact: Increased technical confidence, clear certification path, and practical skills immediately applicable to production projects.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.3-deploybackend/5.3.4-results/","title":"Results &amp; Outputs","tags":[],"description":"","content":"Upon successful deployment, the terminal will display the Outputs section in green. These are critical parameters for connecting the Frontend to the Backend.\nStack Outputs ‚ö†Ô∏è PLEASE COPY AND SAVE THESE VALUES:\n‚úÖ BackendStack Outputs: BackendStack.ApiUrl = https://xyz123.execute-api.us-east-1.amazonaws.com/prod/ BackendStack.UserPoolId = us-east-1_AbCdEfGh BackendStack.UserPoolClientId = 1a2b3c4d5e6f7g8h9i0j BackendStack.IdentityPoolId = us-east-1:12345678-abcd-1234-abcd-1234567890ab BackendStack.ImagesBucket = findnest-images-123456789012 BackendStack.MapName = FindNestMap-123456789012 BackendStack.PlaceIndexName = FindNestPlacesV3-123456789012 BackendStack.RouteCalculatorName = FindNestRoutesV3-123456789012 BackendStack.Region = us-east-1 ‚úÖ MonitoringStack Outputs: MonitoringStack.AlertTopicArn = arn:aws:sns:us-east-1:123456789012:BoardingHouseAlerts MonitoringStack.DashboardUrl = https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=SmartBoardingHouse-Monitoring Output Parameters Explained Parameter Description Usage ApiUrl The endpoint for the Backend API Used by Frontend to make API calls UserPoolId Cognito User Pool identifier Used for user authentication (Login/Register) UserPoolClientId Cognito App Client identifier Used for user authentication (Login/Register) IdentityPoolId Cognito Identity Pool identifier Used for displaying maps on the Frontend ImagesBucket S3 bucket name for images Used for storing and retrieving room images MapName Location Service map name Used for displaying maps on the Frontend application PlaceIndexName Location Service place index Used for geocoding and search functionality RouteCalculatorName Location Service route calculator Used for calculating routes between locations Region AWS region The region where resources are deployed AlertTopicArn SNS Topic ARN for alerts Subscribe to this topic to receive system alerts DashboardUrl CloudWatch Dashboard URL Access monitoring dashboard to view system metrics Next Steps Save these outputs in a secure location. You will need them to configure the Frontend application in the next section.\nYou can also retrieve these outputs later by running cdk deploy again or checking the CloudFormation stack outputs in the AWS Console.\nCongratulations You have successfully deployed a modern Serverless Backend system, integrated with:\n‚úÖ AI (Amazon Bedrock with Claude 3) ‚úÖ Digital Maps (Amazon Location Service) ‚úÖ Strict security permissions (IAM) ‚úÖ Scalable database (DynamoDB) ‚úÖ User authentication (Cognito) ‚úÖ RESTful API (API Gateway + Lambda) ‚úÖ Comprehensive monitoring (CloudWatch Dashboard \u0026amp; Alarms) ‚úÖ Real-time alerts (SNS Notifications) "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-seedingdata/","title":"Seeding Data","tags":[],"description":"","content":"Why seed data? Your infrastructure is ready, but the database is empty. You need a \u0026ldquo;Super Admin\u0026rdquo; account to manage the system.\nWe will run a Node.js script that connects directly to AWS to:\nCreate a User in Cognito User Pool Add that user to the Admins Group Create a corresponding profile in the DynamoDB UserProfiles table Content Setup the Script Configure Environment Run the Script Verification "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/5.5.4-test-public-access/","title":"Test Public Access","tags":[],"description":"","content":"Test Case 3: Public Access (Listings) Finally, verify that public users can view listings without logging in.\nThis validates that your API has both protected and public routes working correctly.\nRequest Details Method: GET\nURL: \u0026lt;ApiUrl\u0026gt;/listings\nHeaders: None required (public endpoint)\nExecute with cURL curl \u0026lt;YOUR_API_URL\u0026gt;/listings Example:\ncurl https://xyz123.execute-api.us-east-1.amazonaws.com/prod/listings Expected Response (200 OK) You should see an array containing the listing you just created in Test Case 2:\n{ \u0026#34;data\u0026#34;: [ { \u0026#34;listingId\u0026#34;: \u0026#34;listing_12345...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;], \u0026#34;createdAt\u0026#34;: \u0026#34;2023-10-27T10:30:00Z\u0026#34;, \u0026#34;landlordId\u0026#34;: \u0026#34;...\u0026#34; } ], \u0026#34;message\u0026#34;: \u0026#34;Listings retrieved successfully\u0026#34; } If you can see the listing without authentication, your public API route is working correctly!\nWhat This Test Validates ‚úÖ Public Access: Unauthenticated users can view listings ‚úÖ DynamoDB Read: Lambda successfully reads data from DynamoDB ‚úÖ Data Retrieval: Previously created data is accessible ‚úÖ API Routing: Both protected and public routes coexist properly Additional Tests You can also test with query parameters:\nSearch by location:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?location=District%201\u0026#34; Filter by price range:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?minPrice=3000000\u0026amp;maxPrice=6000000\u0026#34; Pagination:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?page=1\u0026amp;limit=10\u0026#34; Using a Browser Since this is a GET request, you can simply open the URL in your web browser:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/listings The browser will display the JSON response directly.\nTroubleshooting 404 Not Found: Verify the endpoint path is correct (/listings not /listing) Empty Array: No listings in database yet, create one first (Test Case 2) 500 Internal Server Error: Check Lambda logs for DynamoDB read permission issues CORS Error (in browser): Check API Gateway CORS configuration Use a browser extension like JSON Formatter for better readability of JSON responses in the browser.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.4-seedingdata/5.4.4-verification/","title":"Verification","tags":[],"description":"","content":"Let\u0026rsquo;s ensure the user exists in both the Authentication service and the Database.\nA. Check Cognito (Authentication) Go to Amazon Cognito Console Select User Pool FindNestUsers Go to Users tab You should see your superadmin with status Confirmed.\nClick on the user and verify they are in the Admins group.\nThe user should have:\nStatus: CONFIRMED (not FORCE_CHANGE_PASSWORD) Group membership: Admins B. Check DynamoDB (Data) Go to DynamoDB Console Select the table UserProfiles Click Explore table items Search for the item where userId matches the ID from the script output.\nYou should see the profile data with userType: \u0026quot;admin\u0026quot;.\nVerification Checklist Confirm the following before proceeding:\n‚úÖ User exists in Cognito User Pool with status CONFIRMED ‚úÖ User is a member of the Admins group ‚úÖ User profile exists in DynamoDB UserProfiles table ‚úÖ Profile has userType set to \u0026quot;admin\u0026quot; ‚úÖ All user attributes (email, fullName) are correctly populated Next Steps You are now ready to log in and test the system!\nYour Super Admin account is ready to use. You can now authenticate with the API using these credentials.\nKeep your admin credentials secure. Consider using AWS Secrets Manager for production environments.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Completely present at hackathon demo day Continue to complete the workshop Tasks carried out: Day Task Date Status Mon - Learn Coursera - Prepare for hackathon presentation 11/24/2025 ‚úÖ Done Tue - Present online at hackathon demo day - Create function and connect API to web (RoomList, RoomConfig) 11/25/2025 ‚úÖ Done Wed - Create landing page - Change UI for project - Create function UserList 11/26/2025 ‚úÖ Done Thu - Learn about deploy github page - Redesign the flow of project 11/27/2025 ‚úÖ Done Fri - Implement new admin dashboard and sign up 11/28/2025 ‚úÖ Done Achievements: ‚úÖ Successfully present at hackathon demo day ‚úÖ Successfully redesign the flow of project ‚úÖ Successfully implement new admin dashboard and sign up Key Learnings: Hackathon presentation skills API integration for real-time applications GitHub Pages deployment Admin dashboard design patterns Project flow optimization Challenges: Hackathon presentation preparation ‚Üí Created clear demo script API connection timing issues ‚Üí Implemented proper async handling GitHub Pages deployment ‚Üí Configured proper build settings Summary: Completed final week with successful Hackathon presentation and continued workshop improvements. Enhanced project with better UI/UX and admin features.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/","title":"System Validation","tags":[],"description":"","content":"Testing Strategy We have successfully deployed the backend and seeded the admin user. Now, we need to validate the system functionality.\nWe will simulate a Client Application (like a React Native app) making requests to your Serverless Backend.\nTools: You can use Postman, Thunder Client (VS Code Extension), or simply cURL in your terminal.\nContent Retrieve Configuration Test Admin Login Test Create Listing Test Public Access Verify Resources "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-systemvalidation/5.5.5-verify-resources/","title":"Verify Resources","tags":[],"description":"","content":"Verify Resources in Console If all requests succeed, let\u0026rsquo;s verify the data persistence in AWS Console.\nThis confirms that data is actually being stored in DynamoDB, not just returned from Lambda\u0026rsquo;s memory.\nStep 1: Navigate to DynamoDB Console Go to DynamoDB Console Select Tables from the left sidebar You should see all the tables created by your CDK stack:\nBoardingHouseListings UserProfiles OTPVerifications UserFavorites SupportRequests SearchHistory UserPreferences Step 2: Explore the Listings Table Click on the BoardingHouseListings table Click Explore table items button You should see the listing you created in Test Case 2.\nStep 3: Verify the Data Check that the record contains:\nlistingId: Auto-generated unique identifier title: \u0026ldquo;Luxury Apartment in District 1\u0026rdquo; description: \u0026ldquo;Full furniture, near manufacturing hub\u0026rdquo; price: 5000000 address: \u0026ldquo;123 Le Loi, District 1, HCMC\u0026rdquo; area: 45 amenities: Array containing [\u0026ldquo;Wifi\u0026rdquo;, \u0026ldquo;AC\u0026rdquo;, \u0026ldquo;Parking\u0026rdquo;] createdAt: Timestamp of creation landlordId: User ID of the admin who created it Step 4: Verify User Profile Go back to Tables Click on UserProfiles table Click Explore table items You should see the admin user profile created in Section 5.4:\nuserId: Cognito user ID email: Your admin email fullName: Admin\u0026rsquo;s full name userType: \u0026ldquo;admin\u0026rdquo; createdAt: Timestamp Verification Checklist Confirm the following:\n‚úÖ API Gateway is routing requests correctly ‚úÖ Lambda is processing requests and executing business logic ‚úÖ Cognito is authenticating users successfully ‚úÖ DynamoDB is storing and retrieving data correctly ‚úÖ Protected routes require authentication ‚úÖ Public routes are accessible without authentication ‚úÖ IAM permissions allow Lambda to access all necessary services System Architecture Validation You have successfully validated a complete serverless architecture:\nClient (Postman/cURL) ‚Üì API Gateway (REST API) ‚Üì Lambda Function (Node.js) ‚Üì ‚îú‚îÄ‚Üí Cognito (Authentication) ‚îú‚îÄ‚Üí DynamoDB (Data Storage) ‚îú‚îÄ‚Üí SNS (Notifications - not tested) ‚îú‚îÄ‚Üí Bedrock (AI - not tested) ‚îî‚îÄ‚Üí Location Service (Maps - not tested) Congratulations! Your serverless backend is fully functional and ready for frontend integration.\nNext Steps Test additional endpoints (user registration, favorites, search, etc.) Monitor Lambda execution in CloudWatch Logs Set up CloudWatch alarms for errors Integrate with a frontend application Implement additional features (AI chat, map integration, notifications) Keep your API URL and admin credentials secure. Consider using environment variables or secrets management in production.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Completely workshop Tasks carried out: Day Task Date Status Mon - Redo login lambda and web design for login with two type of accounts(Admin, manager) - Create CRUD for admin account 12/01/2025 ‚úÖ Done Tue - Fix bug login for manager - Fix 2 function ListRoom and UpdateRoom - Create chart for data - Create new create room with IoT core 12/02/2025 ‚úÖ Done Wed - Implement a virtual IoT device on my local laptop to receive and send data to AWS 12/03/2025 ‚úÖ Done Thu - Fix some bug in UI and script python for emulating an IoT device 12/04/2025 ‚úÖ Done Fri - Test and optimize web 12/05/2025 ‚úÖ Done Achievements: ‚úÖ Implemented a Python script to simulate a local IoT device, successfully establishing bidirectional MQTT communication (Telemetry \u0026amp; Control) with AWS IoT Core ‚úÖ Completely the workshop Key Learnings: Multi-role authentication implementation AWS IoT Core integration MQTT protocol for device communication Python IoT device simulation Data visualization with charts Challenges: Role-based access control ‚Üí Implemented proper authentication logic IoT Core bidirectional communication ‚Üí Configured MQTT topics correctly Python script debugging ‚Üí Fixed connection and data format issues Summary: Final week focused on completing workshop with IoT integration. Successfully implemented virtual IoT device simulation and enhanced admin features.\n"},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Serverless Backend with AWS CDK Overview In this workshop, you will build a complete Serverless Backend for the FindNest application - a boarding house rental platform. You will use AWS CDK (Cloud Development Kit) to define and deploy your infrastructure as code.\nYou will learn how to:\nDeploy serverless infrastructure using AWS CDK with TypeScript Build RESTful APIs with API Gateway and Lambda Implement authentication with Amazon Cognito Store data in DynamoDB with proper data modeling Integrate AI capabilities with Amazon Bedrock (Claude 3) Add map functionality with Amazon Location Service Apply security best practices with IAM policies Manage infrastructure lifecycle (deploy and cleanup) Architecture You will deploy a modern serverless architecture consisting of:\nAPI Gateway - RESTful API endpoints AWS Lambda - Serverless compute (Node.js) Amazon DynamoDB - NoSQL database (7 tables) Amazon Cognito - User authentication and authorization Amazon S3 - Image storage Amazon Location Service - Maps and geocoding Amazon Bedrock - AI/ML with Claude 3 Amazon SNS - SMS notifications CloudWatch Logs - Monitoring and logging Content Workshop Overview Prerequisites Deploy Backend Seeding Data System Validation Clean Up Resources "},{"uri":"https://hainam1007.github.io/aws-ojt-report/5-workshop/5.6-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Why is cleanup important? In a Cloud environment, you pay for what you provision. Even though Serverless services like Lambda and DynamoDB (On-Demand) have a generous Free Tier or low idle costs, other resources might incur charges over time:\nS3 Storage: You pay for the data stored in buckets Amazon Location Service: Storing Place Indexes or using Maps CloudWatch Logs: Stored log data To prevent unexpected billing, always clean up your environment after completing a workshop.\nContent Destroy the Stack Verify Resource Deletion Workshop Conclusion "},{"uri":"https://hainam1007.github.io/aws-ojt-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8, 2025 to December 12, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learning about AWS services, through which I improved my skills in cloud computing, AWS services architecture, serverless technologies, cloud infrastructure management, and expanded my understanding of cloud technologies and best practices.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚òê ‚úÖ ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚òê ‚úÖ ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚úÖ ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚òê ‚úÖ 8 Teamwork Working effectively with colleagues and participating in teams ‚òê ‚úÖ ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚òê ‚úÖ ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚òê ‚úÖ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚òê ‚úÖ 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement 1. Communication Skills\nTechnical Presentation: Need to practice explaining complex AWS concepts in simpler terms for non-technical audiences Written Documentation: Should improve clarity and structure when writing technical reports and documentation Proactive Updates: Need to report progress and blockers more regularly without waiting to be asked English Proficiency: Continue improving technical English vocabulary for better communication in international environment 2. Problem-Solving \u0026amp; Critical Thinking\nRoot Cause Analysis: Should develop deeper analytical skills to identify underlying issues rather than just symptoms Solution Design: Need to evaluate multiple approaches before choosing implementation, considering trade-offs and long-term impacts Independent Troubleshooting: Must reduce reliance on mentor for debugging and try systematic problem-solving approaches first Creative Thinking: Should explore alternative solutions and challenge conventional approaches when appropriate 3. Technical Depth \u0026amp; Breadth\nAWS Service Expertise: While familiar with many services, need deeper understanding of advanced features and best practices Infrastructure as Code: Should gain more hands-on practice with CloudFormation/CDK beyond basic examples Security Best Practices: Need to strengthen knowledge of AWS security, IAM policies, and compliance requirements Performance Optimization: Should learn more about cost optimization, performance tuning, and resource sizing 4. Time Management \u0026amp; Productivity\nTask Prioritization: Need to improve ability to prioritize tasks based on urgency and business impact Deadline Management: Should plan work better to avoid last-minute rushes and ensure quality Focus \u0026amp; Efficiency: Must minimize distractions and develop strategies to maintain concentration during complex tasks Learning Balance: Need to better balance structured learning time with hands-on project work 5. Initiative \u0026amp; Ownership\nProactive Learning: Should identify knowledge gaps independently and create personal learning plans Taking Initiative: Need to volunteer for challenging tasks more often rather than waiting for assignments Following Through: Must improve consistency in following up on tasks and closing open items completely Innovation Mindset: Should bring more improvement suggestions and new ideas to team discussions 6. Teamwork \u0026amp; Collaboration\nKnowledge Sharing: Need to document and share learnings more actively with other team members Asking for Help: Should find better balance between independent work and seeking guidance when truly stuck Code Reviews: Must provide more constructive feedback during code reviews and participate more actively Team Dynamics: Should engage more in team discussions and contribute perspectives beyond assigned tasks 7. Professional Development\nCertification Goals: Need to set clear timeline and study plan for AWS certification exams Industry Awareness: Should stay more current with AWS announcements, new services, and industry trends Networking: Must be more active in building professional relationships within the AWS community Career Planning: Should define clearer long-term career goals and align learning activities accordingly "},{"uri":"https://hainam1007.github.io/aws-ojt-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://hainam1007.github.io/aws-ojt-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://hainam1007.github.io/aws-ojt-report/tags/","title":"Tags","tags":[],"description":"","content":""}]